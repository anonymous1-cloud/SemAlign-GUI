#!/usr/bin/env python3
"""
ç¬¬ä¸‰é˜¶æ®µè®­ç»ƒï¼šçŸ­è¯­çº§å¯¹æ¯”å­¦ä¹ 
å»ºç«‹çŸ­è¯­-token âŸ· å›¾åƒ-patchçš„åŒå‘ç»†ç²’åº¦å¯¹åº”
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.amp import autocast, GradScaler
from pathlib import Path
import numpy as np
import time
import os
import sys
from tqdm import tqdm
import wandb
import json
from datetime import datetime

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config import config
from dataloder import create_data_loader
from model3 import Stage3PhraseContrastiveModel
from memory import memory_monitor

# ============ ç¬¬ä¸‰é˜¶æ®µç‰¹å®šé…ç½® ============
# è¿™äº›é…ç½®ç›´æ¥åœ¨train3.pyä¸­å®šä¹‰ï¼Œé¿å…ä¿®æ”¹åŸæœ‰çš„config.py
STAGE3_CONFIG = {
    'stage3_epochs': 15,
    'max_phrases_per_sample': 5,
    'phrase_contrastive_temp': 0.07,
    'phrase_hidden_dim': 768,  # ä¿®æ”¹ä¸º768ï¼Œä¸ç¬¬äºŒé˜¶æ®µä¸€è‡´
    'learning_rate_stage3': 3e-4,
    'weight_decay_stage3': 0.01,
    'grad_accum_steps_stage3': 2,
}


def auto_adjust_batch_size_stage3():
    """æ ¹æ®GPUå†…å­˜è‡ªåŠ¨è°ƒæ•´ç¬¬ä¸‰é˜¶æ®µbatch size"""
    if not config.cuda_available:
        return config.batch_size

    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024 ** 3  # GB

    if gpu_memory >= 24:  # A100 40GB
        batch_size = 8
    elif gpu_memory >= 16:  # V100 16GB
        batch_size = 4
    elif gpu_memory >= 8:  # 2080Ti 11GB
        batch_size = 2
    else:  # ä½ç«¯GPU
        batch_size = 1

    print(f"æ ¹æ®GPUå†…å­˜ ({gpu_memory:.1f}GB) è°ƒæ•´ç¬¬ä¸‰é˜¶æ®µbatch sizeä¸º: {batch_size}")
    return batch_size


class Stage3PhraseTrainer:
    """ç¬¬ä¸‰é˜¶æ®µçŸ­è¯­çº§å¯¹æ¯”å­¦ä¹ è®­ç»ƒå™¨"""

    def __init__(self, stage2_checkpoint: str, use_wandb: bool = True):
        self.config = config
        self.device = config.device
        self.use_wandb = use_wandb

        # åŠ è½½ç¬¬ä¸‰é˜¶æ®µé…ç½®
        self.stage3_config = STAGE3_CONFIG

        print(f"\n{'=' * 60}")
        print("ç¬¬ä¸‰é˜¶æ®µè®­ç»ƒï¼šçŸ­è¯­çº§å¯¹æ¯”å­¦ä¹ ")
        print(f"{'=' * 60}")
        print(f"è®¾å¤‡: {self.device}")
        print(f"ç›®æ ‡: å»ºç«‹çŸ­è¯­â†”Patchç»†ç²’åº¦å¯¹åº”")
        print(f"Stage2éšè—ç»´åº¦: {self.config.hidden_dim}")
        print(f"Stage3é…ç½®çš„éšè—ç»´åº¦: {self.stage3_config['phrase_hidden_dim']}")

        # æ£€æŸ¥æ£€æŸ¥ç‚¹
        if not Path(stage2_checkpoint).exists():
            print(f"âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ°stage2æ£€æŸ¥ç‚¹ {stage2_checkpoint}")
            print("å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„Stage2æ¨¡å‹")

        self.stage2_checkpoint = stage2_checkpoint

        # è°ƒæ•´é…ç½® - ç¬¬ä¸‰é˜¶æ®µéœ€è¦æ›´å¤šå†…å­˜
        if self.config.cuda_available:
            self.config.batch_size = auto_adjust_batch_size_stage3()

        print(f"\nç¬¬ä¸‰é˜¶æ®µé…ç½®:")
        print(f"  è®­ç»ƒè½®æ•°: {self.stage3_config['stage3_epochs']}")
        print(f"  æ‰¹æ¬¡å¤§å°: {self.config.batch_size}")
        print(f"  å­¦ä¹ ç‡: {self.stage3_config['learning_rate_stage3']}")
        print(f"  æ¢¯åº¦ç´¯ç§¯: {self.stage3_config['grad_accum_steps_stage3']}")
        print(f"  æœ€å¤§çŸ­è¯­æ•°: {self.stage3_config['max_phrases_per_sample']}")
        print(f"  çŸ­è¯­éšè—ç»´åº¦: {self.stage3_config['phrase_hidden_dim']}")
        print(f"  å¯¹æ¯”æ¸©åº¦: {self.stage3_config['phrase_contrastive_temp']}")

        # ============ åˆå§‹åŒ–æ¨¡å‹ ============
        print("\nåˆå§‹åŒ–çŸ­è¯­çº§å¯¹æ¯”å­¦ä¹ æ¨¡å‹...")
        memory_monitor.print_memory_stats("åˆå§‹åŒ–å‰")

        # åˆ›å»ºæ¨¡å‹æ—¶ä½¿ç”¨åŸæœ‰é…ç½®ï¼Œä¸è¦ä¿®æ”¹hidden_dim
        self.model = Stage3PhraseContrastiveModel(
            stage2_checkpoint=self.stage2_checkpoint,
            config=self.config  # ç›´æ¥ä½¿ç”¨åŸæœ‰é…ç½®
        ).to(self.device)

        # æ‰“å°å‚æ•°ç»Ÿè®¡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        frozen_params = total_params - trainable_params

        print(f"æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        print(f"  æ€»å‚æ•°: {total_params:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"  å†»ç»“å‚æ•°: {frozen_params:,}")
        print(f"  è®­ç»ƒæ¯”ä¾‹: {trainable_params / total_params * 100:.1f}%")

        # ============ ä¼˜åŒ–å™¨é…ç½® ============
        # åˆ†å±‚å­¦ä¹ ç‡
        phrase_params = []
        patch_params = []
        contrastive_params = []
        other_params = []

        for name, param in self.model.named_parameters():
            if not param.requires_grad:
                continue

            if 'phrase_encoder' in name:
                phrase_params.append(param)
            elif 'patch_encoder' in name:
                patch_params.append(param)
            elif 'contrastive_module' in name:
                contrastive_params.append(param)
            elif 'phrase_projection' in name:
                other_params.append(param)  # çŸ­è¯­æŠ•å½±å±‚
            elif 'visual_adapter' in name:
                other_params.append(param)  # è§†è§‰é€‚é…å™¨
            elif 'visualization_head' in name:
                other_params.append(param)  # å¯è§†åŒ–å¤´

        print(f"\nä¼˜åŒ–å™¨å‚æ•°åˆ†ç»„:")
        print(f"  çŸ­è¯­ç¼–ç å™¨å‚æ•°: {len(phrase_params)}å±‚")
        print(f"  Patchç¼–ç å™¨å‚æ•°: {len(patch_params)}å±‚")
        print(f"  å¯¹æ¯”å­¦ä¹ å‚æ•°: {len(contrastive_params)}å±‚")
        print(f"  å…¶ä»–å‚æ•°: {len(other_params)}å±‚")

        self.optimizer = optim.AdamW([
            {
                'params': phrase_params,
                'lr': self.stage3_config['learning_rate_stage3'],
                'weight_decay': self.stage3_config['weight_decay_stage3']
            },
            {
                'params': patch_params,
                'lr': self.stage3_config['learning_rate_stage3'] * 0.67,  # 2e-4
                'weight_decay': self.stage3_config['weight_decay_stage3'] * 0.1
            },
            {
                'params': contrastive_params,
                'lr': self.stage3_config['learning_rate_stage3'] * 0.33,  # 1e-4
                'weight_decay': self.stage3_config['weight_decay_stage3'] * 0.1
            },
            {
                'params': other_params,
                'lr': self.stage3_config['learning_rate_stage3'] * 0.33,
                'weight_decay': self.stage3_config['weight_decay_stage3']
            }
        ])

        # ============ æ•°æ®åŠ è½½å™¨ ============
        print(f"\nåŠ è½½æ•°æ®é›†...")
        self.train_loader = create_data_loader('train', self.config, is_stage1=False)
        self.val_loader = create_data_loader('val', self.config, is_stage1=False)

        total_steps = self.stage3_config['stage3_epochs'] * len(self.train_loader)

        print(f"æ•°æ®é›†ç»Ÿè®¡:")
        print(f"  è®­ç»ƒé›†: {len(self.train_loader.dataset)} æ ·æœ¬")
        print(f"  éªŒè¯é›†: {len(self.val_loader.dataset)} æ ·æœ¬")
        print(f"  æ€»è®­ç»ƒæ­¥æ•°: {total_steps}")

        # ============ å­¦ä¹ ç‡è°ƒåº¦å™¨ ============
        self.scheduler = optim.lr_scheduler.OneCycleLR(
            self.optimizer,
            max_lr=[
                self.stage3_config['learning_rate_stage3'],  # phrase_params
                self.stage3_config['learning_rate_stage3'] * 0.67,  # patch_params
                self.stage3_config['learning_rate_stage3'] * 0.33,  # contrastive_params
                self.stage3_config['learning_rate_stage3'] * 0.33  # other_params
            ],
            total_steps=total_steps,
            pct_start=0.1,
            anneal_strategy='cos'
        )

        # ============ æ··åˆç²¾åº¦è®­ç»ƒ ============
        if self.config.mixed_precision and self.config.cuda_available:
            self.scaler = GradScaler('cuda')
            print("å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ")
        else:
            self.scaler = None
            print("ç¦ç”¨æ··åˆç²¾åº¦è®­ç»ƒ")

        # ============ æŸå¤±å‡½æ•° ============
        # å¯¹æ¯”å­¦ä¹ æŸå¤±å·²ç»åœ¨æ¨¡å‹ä¸­è®¡ç®—
        # æ·»åŠ è¾…åŠ©å¯¹é½æŸå¤±
        self.alignment_loss_fn = nn.BCEWithLogitsLoss(reduction='mean')

        # ============ è¾“å‡ºç›®å½• ============
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.stage_dir = self.config.output_dir / f"stage3{timestamp}"
        self.checkpoint_dir = self.stage_dir / "checkpoints"
        self.log_dir = self.stage_dir / "logs"
        self.viz_dir = self.stage_dir / "visualizations"

        # åˆ›å»ºç›®å½•
        for dir_path in [self.stage_dir, self.checkpoint_dir, self.log_dir, self.viz_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)

        print(f"\nè¾“å‡ºç›®å½•: {self.stage_dir}")

        # ============ è®­ç»ƒçŠ¶æ€ ============
        self.global_step = 0
        self.current_epoch = 0
        self.best_val_loss = float('inf')
        self.best_alignment_score = 0.0

        # è®­ç»ƒå†å²
        self.train_history = []
        self.val_history = []

        # çŸ­è¯­ç»Ÿè®¡
        self.phrase_stats = {
            'total_phrases': 0,
            'avg_phrases_per_sample': 0,
            'phrase_types': {'addition': 0, 'removal': 0, 'movement': 0}
        }

        memory_monitor.print_memory_stats("åˆå§‹åŒ–å")

        # ============ åˆå§‹åŒ–Wandb ============
        if self.use_wandb:
            try:
                # åˆå¹¶é…ç½®
                wandb_config = self.config.__dict__.copy()
                wandb_config.update(self.stage3_config)

                wandb.init(
                    project="gui-change-detection",
                    name=f"stage3{timestamp}",
                    config=wandb_config,
                    dir=str(self.log_dir)
                )
                wandb.watch(self.model, log='all', log_freq=50)
                print("Wandbåˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"Wandbåˆå§‹åŒ–å¤±è´¥: {e}")
                self.use_wandb = False

    def compute_total_loss(self, outputs, batch):
        """è®¡ç®—æ€»æŸå¤± - ä¿®å¤è®¾å¤‡ä¸åŒ¹é…é—®é¢˜"""
        losses = {}

        try:
            # è·å–è®¾å¤‡ä¿¡æ¯
            device = outputs.get('patch_features', torch.zeros(1).to(self.device)).device

            # 1. å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼ˆä¸»è¦æŸå¤±ï¼‰
            contrastive_loss = outputs.get('total_contrastive_loss', torch.tensor(0.0, device=device))
            losses['contrastive'] = contrastive_loss

            # 2. çŸ­è¯­åˆ°patchæŸå¤±
            phrase_to_patch = outputs.get('loss_phrase_to_patch', torch.tensor(0.0, device=device))
            losses['phrase_to_patch'] = phrase_to_patch

            # 3. patchåˆ°çŸ­è¯­æŸå¤±
            patch_to_phrase = outputs.get('loss_patch_to_phrase', torch.tensor(0.0, device=device))
            losses['patch_to_phrase'] = patch_to_phrase

            # 4. å¯¹é½ä¸€è‡´æ€§æŸå¤±ï¼ˆå¯é€‰ï¼‰
            if 'stage2_features' in outputs and outputs['stage2_features'] is not None:
                alignment_scores = outputs['stage2_features'].get('alignment_scores')
                if alignment_scores is not None and 'has_change' in batch:
                    # ç¡®ä¿æ ‡ç­¾åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                    alignment_labels = batch['has_change'].unsqueeze(1).float().to(device)
                    alignment_loss = self.alignment_loss_fn(alignment_scores, alignment_labels)
                    losses['alignment'] = alignment_loss * 0.1  # è¾ƒå°çš„æƒé‡
                else:
                    losses['alignment'] = torch.tensor(0.0, device=device)
            else:
                losses['alignment'] = torch.tensor(0.0, device=device)

            # 5. æ€»æŸå¤±
            total_loss = sum(losses.values())
            losses['total'] = total_loss

            # 6. çŸ­è¯­ç»Ÿè®¡
            num_phrases = outputs.get('num_phrases', 0)
            parsed_phrases = outputs.get('parsed_phrases', [])

            if num_phrases > 0 and parsed_phrases:
                for phrases in parsed_phrases:
                    for phrase in phrases:
                        phrase_type = phrase.get('type', 'unknown')
                        if phrase_type in self.phrase_stats['phrase_types']:
                            self.phrase_stats['phrase_types'][phrase_type] += 1

        except Exception as e:
            print(f"æŸå¤±è®¡ç®—é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            # è¿”å›é»˜è®¤æŸå¤±
            device = outputs.get('patch_features', torch.zeros(1).to(self.device)).device
            total_loss = torch.tensor(0.0, device=device, requires_grad=True)
            losses = {'total': total_loss}

        return total_loss, losses

    def train_epoch(self, epoch: int):
        """è®­ç»ƒä¸€ä¸ªepoch - ä¿®å¤æ¢¯åº¦ç¼©æ”¾å™¨ä½¿ç”¨"""
        self.model.train()
        epoch_losses = {}
        total_phrases = 0
        total_samples = 0

        # è®¾ç½®æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        grad_accum_steps = self.stage3_config['grad_accum_steps_stage3']

        # è®°å½•å½“å‰æ˜¯å¦å·²ç»unscaleè¿‡
        unscaled_in_step = False

        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch + 1}/{self.stage3_config['stage3_epochs']}")

        for batch_idx, batch in enumerate(pbar):
            if batch is None:
                continue

            try:
                # å‡†å¤‡æ•°æ®
                ref_image = batch['ref_image'].to(self.device, non_blocking=True)
                tar_image = batch['tar_image'].to(self.device, non_blocking=True)
                text_tokens = batch['text_tokens'].to(self.device, non_blocking=True)
                ref_components = batch['ref_components'].to(self.device, non_blocking=True)
                tar_components = batch['tar_components'].to(self.device, non_blocking=True)

                # è·å–differ_text
                differ_texts = batch.get('text', [''] * len(ref_image))

                # ç¡®ä¿ç»„ä»¶æ•°æ®åœ¨æœ‰æ•ˆèŒƒå›´å†…
                if ref_components is not None:
                    ref_components[:, :, 0] = torch.clamp(ref_components[:, :, 0], 0, 19)
                if tar_components is not None:
                    tar_components[:, :, 0] = torch.clamp(tar_components[:, :, 0], 0, 19)

                # å‰å‘ä¼ æ’­
                if self.scaler:
                    with autocast('cuda'):
                        outputs = self.model(
                            ref_image, tar_image, text_tokens,
                            ref_components, tar_components, differ_texts
                        )
                        total_loss, loss_dict = self.compute_total_loss(outputs, batch)
                else:
                    outputs = self.model(
                        ref_image, tar_image, text_tokens,
                        ref_components, tar_components, differ_texts
                    )
                    total_loss, loss_dict = self.compute_total_loss(outputs, batch)

                # æ£€æŸ¥æŸå¤±æœ‰æ•ˆæ€§
                if not total_loss.requires_grad or torch.isnan(total_loss) or torch.isinf(total_loss):
                    print(f"è­¦å‘Š: æ— æ•ˆæŸå¤±ï¼Œè·³è¿‡æ‰¹æ¬¡ {batch_idx}")
                    self.optimizer.zero_grad(set_to_none=True)
                    unscaled_in_step = False
                    continue

                # æ¢¯åº¦ç´¯ç§¯
                loss = total_loss / grad_accum_steps

                if self.scaler:
                    self.scaler.scale(loss).backward()
                else:
                    loss.backward()

                # æ¢¯åº¦ç´¯ç§¯æ­¥éª¤
                if (batch_idx + 1) % grad_accum_steps == 0:
                    # æ¢¯åº¦è£å‰ª - åªåœ¨æ··åˆç²¾åº¦è®­ç»ƒä¸”éœ€è¦è£å‰ªæ—¶æ‰unscale
                    if self.scaler and not unscaled_in_step:
                        self.scaler.unscale_(self.optimizer)
                        unscaled_in_step = True

                    # æ£€æŸ¥æ¢¯åº¦
                    grad_norm = 0.0
                    grad_params = 0
                    for param in self.model.parameters():
                        if param.grad is not None:
                            grad_norm += param.grad.norm().item() ** 2
                            grad_params += 1

                    if grad_params > 0:
                        grad_norm = grad_norm ** 0.5
                        if grad_norm > 1.0:
                            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                    # ä¼˜åŒ–å™¨æ­¥éª¤
                    if self.scaler:
                        self.scaler.step(self.optimizer)
                        self.scaler.update()
                    else:
                        self.optimizer.step()

                    self.optimizer.zero_grad(set_to_none=True)
                    unscaled_in_step = False  # é‡ç½®æ ‡å¿—
                    self.scheduler.step()
                    self.global_step += 1

                # ç´¯ç§¯æŸå¤±
                for key, value in loss_dict.items():
                    if key not in epoch_losses:
                        epoch_losses[key] = []
                    epoch_losses[key].append(value.item())

                # æ›´æ–°çŸ­è¯­ç»Ÿè®¡
                num_phrases = outputs.get('num_phrases', 0)
                total_phrases += num_phrases
                total_samples += ref_image.shape[0]

                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'loss': f"{total_loss.item():.4f}",
                    'phrases': num_phrases,
                    'contrastive': f"{loss_dict.get('contrastive', 0):.4f}",
                    'lr': f"{self.scheduler.get_last_lr()[0]:.2e}",
                    'step': self.global_step
                })

                # å®šæœŸè®°å½•å’Œæ¸…ç†
                if self.global_step % 50 == 0:
                    if self.use_wandb:
                        wandb.log({
                            'train/step_loss': total_loss.item(),
                            'train/step_contrastive_loss': loss_dict.get('contrastive', 0),
                            'train/step_phrase_to_patch': loss_dict.get('phrase_to_patch', 0),
                            'train/step_patch_to_phrase': loss_dict.get('patch_to_phrase', 0),
                            'train/learning_rate': self.scheduler.get_last_lr()[0],
                            'train/step': self.global_step,
                            'train/num_phrases': num_phrases
                        })

                    memory_monitor.clear_cache()

            except Exception as e:
                print(f"æ‰¹æ¬¡ {batch_idx} å¤„ç†é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
                self.optimizer.zero_grad(set_to_none=True)
                unscaled_in_step = False
                continue

        # è®¡ç®—å¹³å‡æŸå¤±
        avg_losses = {}
        for key, values in epoch_losses.items():
            if values:
                avg_losses[key] = np.mean(values)
            else:
                avg_losses[key] = 0.0

        # æ›´æ–°çŸ­è¯­ç»Ÿè®¡
        if total_samples > 0:
            self.phrase_stats['total_phrases'] += total_phrases
            self.phrase_stats['avg_phrases_per_sample'] = total_phrases / total_samples

        return avg_losses

    def save_visualization(self, batch, outputs, epoch, batch_idx):
        """ä¿å­˜å¯è§†åŒ–ç¤ºä¾‹"""
        try:
            import matplotlib.pyplot as plt
            from matplotlib.patches import Rectangle

            batch_size = min(2, batch['ref_image'].shape[0])  # åªä¿å­˜å‰2ä¸ªæ ·æœ¬

            for i in range(batch_size):
                fig, axes = plt.subplots(2, 3, figsize=(15, 10))

                # å‚è€ƒå›¾åƒ
                ref_img = batch['ref_image'][i].cpu().permute(1, 2, 0).numpy()
                axes[0, 0].imshow(ref_img)
                axes[0, 0].set_title('Reference Image')
                axes[0, 0].axis('off')

                # ç›®æ ‡å›¾åƒ
                tar_img = batch['tar_image'][i].cpu().permute(1, 2, 0).numpy()
                axes[0, 1].imshow(tar_img)
                axes[0, 1].set_title('Target Image')
                axes[0, 1].axis('off')

                # Mask
                mask = batch['mask'][i].cpu().numpy()
                axes[0, 2].imshow(mask, cmap='hot')
                axes[0, 2].set_title('Change Mask')
                axes[0, 2].axis('off')

                # çŸ­è¯­-Patchå¯¹åº”å…³ç³»
                parsed_phrases = outputs.get('parsed_phrases', [])
                if i < len(parsed_phrases):
                    phrases = parsed_phrases[i]
                    correspondences = outputs.get('correspondences', [])

                    # æ˜¾ç¤ºç›®æ ‡å›¾åƒ
                    axes[1, 0].imshow(tar_img)

                    # ç»˜åˆ¶çŸ­è¯­å¯¹åº”çš„patch
                    for phrase_idx, phrase in enumerate(phrases):
                        # æŸ¥æ‰¾å¯¹åº”å…³ç³»
                        for corr in correspondences:
                            if corr['batch_idx'] == i and corr['phrase_idx'] == phrase_idx:
                                for patch in corr['top_patches'][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                                    bbox = patch['bbox']
                                    rect = Rectangle(
                                        (bbox[0], bbox[1]), bbox[2] - bbox[0], bbox[3] - bbox[1],
                                        linewidth=2, edgecolor='r', facecolor='none'
                                    )
                                    axes[1, 0].add_patch(rect)

                                # æ·»åŠ çŸ­è¯­æ–‡æœ¬
                                axes[1, 0].text(
                                    10, 20 + phrase_idx * 20,
                                    f"{phrase.get('component', '')}",
                                    color='white', backgroundcolor='red',
                                    fontsize=8
                                )
                                break

                    axes[1, 0].set_title('Phrase-Patch Correspondence')
                    axes[1, 0].axis('off')

                # çŸ­è¯­çƒ­åŠ›å›¾
                phrase_heatmaps = outputs.get('phrase_heatmaps', [])
                if phrase_heatmaps:
                    # åˆå¹¶æ‰€æœ‰çŸ­è¯­çš„çƒ­åŠ›å›¾
                    combined_heatmap = torch.zeros(224, 224, device=self.device)
                    for heatmap in phrase_heatmaps:
                        if heatmap.shape == (224, 224):
                            combined_heatmap = torch.max(combined_heatmap, heatmap)

                    axes[1, 1].imshow(tar_img, alpha=0.7)
                    axes[1, 1].imshow(combined_heatmap.cpu().numpy(), cmap='jet', alpha=0.5)
                    axes[1, 1].set_title('Combined Phrase Heatmap')
                    axes[1, 1].axis('off')

                # æ–‡æœ¬æè¿°
                text = batch.get('text', [''])[i] if i < len(batch.get('text', [])) else ''
                axes[1, 2].text(0.1, 0.9, 'Text Description:', fontsize=12, fontweight='bold')
                axes[1, 2].text(0.1, 0.1, text[:100] + ('...' if len(text) > 100 else ''),
                                fontsize=9, verticalalignment='bottom')
                axes[1, 2].axis('off')

                plt.tight_layout()

                # ä¿å­˜å›¾åƒ
                viz_path = self.viz_dir / f"epoch_{epoch}_step_{self.global_step}_sample_{i}.png"
                plt.savefig(viz_path, dpi=150, bbox_inches='tight')
                plt.close()

                # ä¿å­˜åˆ°wandb
                if self.use_wandb:
                    wandb.log({
                        f"visualizations/sample_{i}": wandb.Image(str(viz_path)),
                        'step': self.global_step
                    })

                print(f"âœ… ä¿å­˜å¯è§†åŒ–: {viz_path}")

        except Exception as e:
            print(f"å¯è§†åŒ–ä¿å­˜å¤±è´¥: {e}")

    @torch.no_grad()
    def validate(self):
        """éªŒè¯è¿‡ç¨‹ - ä¿®å¤è®¾å¤‡é—®é¢˜"""
        self.model.eval()
        val_losses = []
        val_metrics = []

        print(f"\néªŒè¯æ¨¡å‹ (Step {self.global_step})...")
        print(f"é‡‡æ ·éªŒè¯æ‰¹æ¬¡è¿›è¡Œè¯¦ç»†åˆ†æ...")

        # åªéªŒè¯éƒ¨åˆ†æ‰¹æ¬¡ä»¥èŠ‚çœæ—¶é—´
        max_val_batches = min(20, len(self.val_loader))
        val_iterator = iter(self.val_loader)

        for batch_idx in tqdm(range(max_val_batches), desc="Validation"):
            try:
                batch = next(val_iterator)
                if batch is None:
                    continue

                ref_image = batch['ref_image'].to(self.device, non_blocking=True)
                tar_image = batch['tar_image'].to(self.device, non_blocking=True)
                text_tokens = batch['text_tokens'].to(self.device, non_blocking=True)
                ref_components = batch['ref_components'].to(self.device, non_blocking=True)
                tar_components = batch['tar_components'].to(self.device, non_blocking=True)
                differ_texts = batch.get('text', [''] * len(ref_image))

                outputs = self.model(
                    ref_image, tar_image, text_tokens,
                    ref_components, tar_components, differ_texts
                )

                # ç¡®ä¿batchæ•°æ®åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
                batch_on_device = {}
                for key, value in batch.items():
                    if torch.is_tensor(value):
                        batch_on_device[key] = value.to(self.device)
                    else:
                        batch_on_device[key] = value

                total_loss, loss_dict = self.compute_total_loss(outputs, batch_on_device)
                val_losses.append(total_loss.item())

                # æ”¶é›†æŒ‡æ ‡
                metrics = {'val_loss': total_loss.item()}
                for k, v in loss_dict.items():
                    if torch.is_tensor(v):
                        metrics[f'val_{k}'] = v.item()
                    else:
                        metrics[f'val_{k}'] = v

                # çŸ­è¯­å¯¹é½è´¨é‡æŒ‡æ ‡
                num_phrases = outputs.get('num_phrases', 0)
                correspondences = outputs.get('correspondences', [])

                if num_phrases > 0 and correspondences:
                    # è®¡ç®—å¹³å‡åŒ¹é…åˆ†æ•°
                    match_scores = [corr['max_score'] for corr in correspondences]
                    avg_match_score = np.mean(match_scores) if match_scores else 0.0

                    metrics['val_avg_match_score'] = avg_match_score
                    metrics['val_num_phrases'] = num_phrases

                val_metrics.append(metrics)

            except Exception as e:
                print(f"éªŒè¯æ‰¹æ¬¡ {batch_idx} é”™è¯¯: {e}")
                continue

        if not val_losses:
            print("è­¦å‘Šï¼šéªŒè¯é›†ä¸ºç©ºæˆ–æ‰€æœ‰æ‰¹æ¬¡éƒ½å¤±è´¥")
            return {'val_loss': float('inf'), 'alignment_score': 0.0}

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_metrics = {}
        if val_metrics:
            for key in val_metrics[0].keys():
                values = [m[key] for m in val_metrics if key in m]
                if values:
                    avg_metrics[key] = np.mean(values)

        avg_val_loss = np.mean(val_losses)
        alignment_score = 1.0 / (avg_val_loss + 1e-8)

        avg_metrics['val_loss'] = avg_val_loss
        avg_metrics['alignment_score'] = alignment_score

        # æ‰“å°é‡è¦æŒ‡æ ‡
        print(f"\néªŒè¯ç»“æœ:")
        print(f"  éªŒè¯æŸå¤±: {avg_val_loss:.4f}")
        print(f"  å¯¹é½åˆ†æ•°: {alignment_score:.4f}")
        print(f"  æ€»çŸ­è¯­æ•°: {self.phrase_stats['total_phrases']}")

        if 'val_avg_match_score' in avg_metrics:
            print(f"  å¹³å‡åŒ¹é…åˆ†æ•°: {avg_metrics['val_avg_match_score']:.4f}")

        # çŸ­è¯­ç±»å‹ç»Ÿè®¡
        print(f"\nçŸ­è¯­ç±»å‹ç»Ÿè®¡:")
        for phrase_type, count in self.phrase_stats['phrase_types'].items():
            print(f"  {phrase_type}: {count}")

        return avg_metrics

    def save_checkpoint(self, is_best: bool = False, suffix: str = ""):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        try:
            checkpoint = {
                'epoch': self.current_epoch,
                'global_step': self.global_step,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'scheduler_state_dict': self.scheduler.state_dict(),
                'best_val_loss': self.best_val_loss,
                'best_alignment_score': self.best_alignment_score,
                'config': self.config.__dict__,
                'stage3_config': self.stage3_config,
                'train_history': self.train_history,
                'val_history': self.val_history,
                'phrase_stats': self.phrase_stats,
                'timestamp': time.time()
            }

            if self.scaler:
                checkpoint['scaler_state_dict'] = self.scaler.state_dict()

            if suffix:
                checkpoint_path = self.checkpoint_dir / f"checkpoint_{suffix}.pt"
            else:
                checkpoint_path = self.checkpoint_dir / f"checkpoint_epoch_{self.current_epoch}_step_{self.global_step}.pt"

            torch.save(checkpoint, checkpoint_path)
            print(f"âœ… ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")

            # ä¿å­˜é…ç½®
            config_path = self.checkpoint_dir / "config.json"
            with open(config_path, 'w') as f:
                config_dict = self.config.__dict__.copy()
                config_dict.update(self.stage3_config)
                json.dump(config_dict, f, indent=2)

            if is_best:
                best_path = self.checkpoint_dir / "best_model.pt"
                torch.save(checkpoint, best_path)
                print(f"ğŸ‰ ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")

                # å¯¼å‡ºä¸ºéƒ¨ç½²æ ¼å¼
                self.export_model()

        except Exception as e:
            print(f"ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")

    def export_model(self):
        """å¯¼å‡ºæ¨¡å‹"""
        try:
            export_path = self.stage_dir / "stage3_model.pth"
            torch.save({
                'model_state_dict': self.model.state_dict(),
                'config': self.config.__dict__,
                'stage3_config': self.stage3_config,
                'phrase_stats': self.phrase_stats,
                'global_step': self.global_step,
                'best_val_loss': self.best_val_loss,
                'best_alignment_score': self.best_alignment_score
            }, export_path)
            print(f"âœ… æ¨¡å‹å¯¼å‡ºåˆ°: {export_path}")
        except Exception as e:
            print(f"æ¨¡å‹å¯¼å‡ºå¤±è´¥: {e}")

    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print(f"\n{'=' * 60}")
        print(f"å¼€å§‹ç¬¬ä¸‰é˜¶æ®µè®­ç»ƒï¼Œå…± {self.stage3_config['stage3_epochs']} ä¸ªepoch")
        print(f"{'=' * 60}")

        start_time = time.time()
        epoch_times = []

        try:
            for epoch in range(self.current_epoch, self.stage3_config['stage3_epochs']):
                self.current_epoch = epoch
                epoch_start_time = time.time()

                print(f"\n{'=' * 50}")
                print(f"Epoch {epoch + 1}/{self.stage3_config['stage3_epochs']}")
                print(f"{'=' * 50}")

                # è®­ç»ƒ
                train_losses = self.train_epoch(epoch)

                if train_losses:
                    print(f"\nEpoch {epoch + 1} è®­ç»ƒå®Œæˆ:")
                    for key, value in train_losses.items():
                        print(f"  {key}: {value:.4f}")

                    # è®°å½•è®­ç»ƒå†å²
                    self.train_history.append({
                        'epoch': epoch,
                        'step': self.global_step,
                        **train_losses
                    })

                # éªŒè¯
                val_metrics = self.validate()

                # è®°å½•éªŒè¯å†å²
                self.val_history.append({
                    'epoch': epoch,
                    'step': self.global_step,
                    **val_metrics
                })

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_metrics['alignment_score'] > self.best_alignment_score:
                    self.best_alignment_score = val_metrics['alignment_score']
                    self.best_val_loss = val_metrics['val_loss']
                    self.save_checkpoint(is_best=True, suffix="best")
                    print(f"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹ï¼å¯¹é½åˆ†æ•°: {self.best_alignment_score:.4f}")

                # ä¿å­˜å®šæœŸæ£€æŸ¥ç‚¹
                if (epoch + 1) % 5 == 0 or epoch == self.stage3_config['stage3_epochs'] - 1:
                    self.save_checkpoint(suffix=f"epoch_{epoch + 1}")

                # è®¡ç®—epochæ—¶é—´
                epoch_time = time.time() - epoch_start_time
                epoch_times.append(epoch_time)
                avg_epoch_time = np.mean(epoch_times) if epoch_times else epoch_time
                remaining_time = avg_epoch_time * (self.stage3_config['stage3_epochs'] - epoch - 1)

                print(f"\nEpoch {epoch + 1} æ—¶é—´: {epoch_time:.1f}s")
                print(f"é¢„è®¡å‰©ä½™æ—¶é—´: {remaining_time / 60:.1f}åˆ†é’Ÿ")

                # è®°å½•åˆ°wandb
                if self.use_wandb:
                    wandb.log({
                        'train/epoch_loss': train_losses.get('total', 0),
                        'train/epoch_contrastive_loss': train_losses.get('contrastive', 0),
                        'val/epoch_loss': val_metrics.get('val_loss', 0),
                        'val/alignment_score': val_metrics.get('alignment_score', 0),
                        'val/epoch': epoch,
                        'train/epoch': epoch
                    })

                print(f"{'=' * 50}")

                # å†…å­˜æ¸…ç†
                memory_monitor.clear_cache()

        except KeyboardInterrupt:
            print("\n\nè®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
            self.save_checkpoint(suffix="interrupted")
        except Exception as e:
            print(f"\n\nè®­ç»ƒå‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            self.save_checkpoint(suffix="error")
        finally:
            self.save_checkpoint(suffix="final")

        # è®­ç»ƒæ€»ç»“
        total_time = time.time() - start_time
        print(f"\n{'=' * 60}")
        print("ç¬¬ä¸‰é˜¶æ®µè®­ç»ƒæ€»ç»“:")
        print(f"  æ€»æ—¶é—´: {total_time / 60:.1f} åˆ†é’Ÿ")
        print(f"  æ€»æ­¥æ•°: {self.global_step}")
        print(f"  æœ€ä½³å¯¹é½åˆ†æ•°: {self.best_alignment_score:.4f}")
        print(f"  æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f}")

        # çŸ­è¯­ç»Ÿè®¡
        print(f"\nçŸ­è¯­ç»Ÿè®¡:")
        print(f"  æ€»çŸ­è¯­æ•°: {self.phrase_stats['total_phrases']}")
        print(f"  å¹³å‡æ¯æ ·æœ¬çŸ­è¯­æ•°: {self.phrase_stats.get('avg_phrases_per_sample', 0):.2f}")
        for phrase_type, count in self.phrase_stats['phrase_types'].items():
            print(f"  {phrase_type}: {count}")

        print(f"{'=' * 60}")

        # å…³é—­wandb
        if self.use_wandb:
            wandb.finish()


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='ç¬¬ä¸‰é˜¶æ®µè®­ç»ƒï¼šçŸ­è¯­çº§å¯¹æ¯”å­¦ä¹ ')
    parser.add_argument('--stage2-checkpoint', type=str, required=True,
                        default="/home/common-dir/result/training_output/stage2_alignment/checkpoints/best_model.pt",
                        help='Stage2æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--resume', type=str, help='ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ')
    parser.add_argument('--no-wandb', action='store_true', default=False, help='ç¦ç”¨wandb')
    parser.add_argument('--epochs', type=int, default=None, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=None, help='batch size')
    parser.add_argument('--lr', type=float, default=None, help='å­¦ä¹ ç‡')
    parser.add_argument('--no-mixed-precision', action='store_true', default=False, help='ç¦ç”¨æ··åˆç²¾åº¦')
    parser.add_argument('--grad-accum-steps', type=int, default=None, help='æ¢¯åº¦ç´¯ç§¯æ­¥æ•°')
    parser.add_argument('--num_workers', type=int, default=None, help='å·¥ä½œå°æ•°é‡')

    args = parser.parse_args()

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Stage3PhraseTrainer(args.stage2_checkpoint, use_wandb=not args.no_wandb)

    # è¦†ç›–è®­ç»ƒå™¨é…ç½®ï¼ˆå¦‚æœæä¾›äº†å‘½ä»¤è¡Œå‚æ•°ï¼‰
    if args.epochs:
        trainer.stage3_config['stage3_epochs'] = args.epochs
    if args.batch_size:
        trainer.config.batch_size = args.batch_size
    if args.lr:
        trainer.stage3_config['learning_rate_stage3'] = args.lr
    if args.grad_accum_steps:
        trainer.stage3_config['grad_accum_steps_stage3'] = args.grad_accum_steps
    if args.no_mixed_precision:
        trainer.config.mixed_precision = False

    print(f"\næœ€ç»ˆç¬¬ä¸‰é˜¶æ®µé…ç½®:")
    print(f"  è®­ç»ƒè½®æ•°: {trainer.stage3_config['stage3_epochs']}")
    print(f"  æ‰¹æ¬¡å¤§å°: {trainer.config.batch_size}")
    print(f"  å­¦ä¹ ç‡: {trainer.stage3_config['learning_rate_stage3']}")
    print(f"  æ¢¯åº¦ç´¯ç§¯: {trainer.stage3_config['grad_accum_steps_stage3']}")
    print(f"  æ··åˆç²¾åº¦: {trainer.config.mixed_precision}")

    # å¤„ç†æ¢å¤è®­ç»ƒ
    if args.resume and Path(args.resume).exists():
        print(f"\nä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {args.resume}")
        try:
            checkpoint = torch.load(args.resume, map_location='cpu')
            trainer.model.load_state_dict(checkpoint['model_state_dict'])
            trainer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            trainer.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            trainer.current_epoch = checkpoint.get('epoch', 0)
            trainer.global_step = checkpoint.get('global_step', 0)
            trainer.best_val_loss = checkpoint.get('best_val_loss', float('inf'))
            trainer.best_alignment_score = checkpoint.get('best_alignment_score', 0.0)
            trainer.train_history = checkpoint.get('train_history', [])
            trainer.val_history = checkpoint.get('val_history', [])
            trainer.phrase_stats = checkpoint.get('phrase_stats', {})

            if trainer.scaler and 'scaler_state_dict' in checkpoint:
                trainer.scaler.load_state_dict(checkpoint['scaler_state_dict'])

            print(f"æ¢å¤æˆåŠŸ: epoch={trainer.current_epoch}, step={trainer.global_step}")
        except Exception as e:
            print(f"æ¢å¤å¤±è´¥: {e}")

    # å¼€å§‹è®­ç»ƒ
    trainer.train()


if __name__ == "__main__":
    main()


