#!/usr/bin/env python3
"""
ä¸‰é˜¶æ®µGUIå˜åŒ–åˆ†æç³»ç»Ÿ - é›†æˆæ¨ç†ç‰ˆ
ä½¿ç”¨è®­ç»ƒå¥½çš„ä¸‰ä¸ªé˜¶æ®µæƒé‡è¿›è¡Œå®Œæ•´åˆ†æ
"""

import torch
import torch.nn as nn
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.patches import FancyArrowPatch
from pathlib import Path
import json
import re
import time
import warnings
from typing import Dict, List, Tuple, Optional, Any
import sys

# æ·»åŠ æ¨¡å‹è·¯å¾„
sys.path.append('.')
warnings.filterwarnings('ignore')

# å¯¼å…¥æ¨¡å‹ç±»
from models import Stage1VisualModel
from model2 import Stage2AlignmentModel
from model3 import Stage3PhraseContrastiveModel


class IntegratedGUIAnalyzer:
    """é›†æˆä¸‰é˜¶æ®µæ¨¡å‹çš„GUIåˆ†æç³»ç»Ÿ"""

    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        print(f"\n{'=' * 60}")
        print("åˆå§‹åŒ–é›†æˆGUIåˆ†æç³»ç»Ÿ")
        print(f"è®¾å¤‡: {self.device}")
        print(f"{'=' * 60}")

        # æ¨¡å‹è·¯å¾„ - æ ¹æ®æ‚¨çš„å®é™…è·¯å¾„ä¿®æ”¹
        STAGE1_MODEL = "/home/common-dir/result/training_output/stage1/checkpoints/best_model.pt"
        STAGE2_MODEL = "/home/common-dir/result/training_output/stage2_alignment/checkpoints/best_model.pt"
        STAGE3_MODEL = "/home/common-dir/result/training_output/stage3_phrase_20260114_053101/checkpoints/checkpoint_final.pt"

        # ============ 1. åˆå§‹åŒ–ç¬¬ä¸€é˜¶æ®µæ¨¡å‹ ============
        print("\n1. åŠ è½½ç¬¬ä¸€é˜¶æ®µæ¨¡å‹...")
        self.stage1_model = Stage1VisualModel(config).to(self.device)

        if Path(STAGE1_MODEL).exists():
            try:
                # å°è¯•ä½¿ç”¨weights_only=True
                checkpoint = torch.load(STAGE1_MODEL, map_location=self.device, weights_only=True)
                state_dict = checkpoint.get('model_state_dict', checkpoint)

                # å¦‚æœåŠ è½½å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
                if state_dict is None:
                    # ä½¿ç”¨pickleç›´æ¥åŠ è½½
                    import pickle
                    with open(STAGE1_MODEL, 'rb') as f:
                        state_dict = pickle.load(f)

            except Exception as e:
                print(f"âš ï¸ æ ‡å‡†åŠ è½½å¤±è´¥ï¼Œå°è¯•å¤‡é€‰æ–¹æ³•: {e}")
                # å¤‡é€‰åŠ è½½æ–¹æ³•
                try:
                    checkpoint = torch.load(STAGE1_MODEL, map_location=self.device,
                                            weights_only=False, pickle_module=pickle)
                    state_dict = checkpoint.get('model_state_dict', checkpoint)
                except:
                    print(f"âŒ æ‰€æœ‰åŠ è½½æ–¹æ³•å‡å¤±è´¥")
                    state_dict = None

            if state_dict is not None:
                self.stage1_model.load_state_dict(state_dict, strict=False)
                print("âœ… ç¬¬ä¸€é˜¶æ®µæ¨¡å‹åŠ è½½æˆåŠŸ")
            else:
                print(f"âš ï¸ æ— æ³•åŠ è½½æ¨¡å‹æƒé‡")
        else:
            print(f"âš ï¸ ç¬¬ä¸€é˜¶æ®µæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {STAGE1_MODEL}")

        self.stage1_model.eval()

        # ============ 2. åˆå§‹åŒ–ç¬¬äºŒé˜¶æ®µæ¨¡å‹ ============
        print("\n2. åŠ è½½ç¬¬äºŒé˜¶æ®µæ¨¡å‹...")
        self.stage2_model = Stage2AlignmentModel(
            stage1_checkpoint="",  # ä¸éœ€è¦stage1æ£€æŸ¥ç‚¹ï¼Œå› ä¸ºå·²ç»åˆå§‹åŒ–äº†æ¨¡å‹
            config=config,
            use_components=True
        ).to(self.device)

        if Path(STAGE2_MODEL).exists():
            checkpoint = torch.load(STAGE2_MODEL, map_location=self.device, weights_only=False)
            state_dict = checkpoint.get('model_state_dict', checkpoint)
            self.stage2_model.load_state_dict(state_dict, strict=False)
            print("âœ… ç¬¬äºŒé˜¶æ®µæ¨¡å‹åŠ è½½æˆåŠŸ")
        else:
            print(f"âš ï¸ ç¬¬äºŒé˜¶æ®µæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {STAGE2_MODEL}")

        self.stage2_model.eval()

        # ============ 3. åˆå§‹åŒ–ç¬¬ä¸‰é˜¶æ®µæ¨¡å‹ ============
        print("\n3. åŠ è½½ç¬¬ä¸‰é˜¶æ®µæ¨¡å‹...")
        self.stage3_model = Stage3PhraseContrastiveModel(
            stage2_checkpoint="",  # ä¸éœ€è¦stage2æ£€æŸ¥ç‚¹
            config=config
        ).to(self.device)

        if Path(STAGE3_MODEL).exists():
            checkpoint = torch.load(STAGE3_MODEL, map_location=self.device, weights_only=False)
            state_dict = checkpoint.get('model_state_dict', checkpoint)
            self.stage3_model.load_state_dict(state_dict, strict=False)
            print("âœ… ç¬¬ä¸‰é˜¶æ®µæ¨¡å‹åŠ è½½æˆåŠŸ")
        else:
            print(f"âš ï¸ ç¬¬ä¸‰é˜¶æ®µæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {STAGE3_MODEL}")

        self.stage3_model.eval()

        # ============ 4. æ•°æ®é¢„å¤„ç†å·¥å…· ============
        self.component_types = {
            'TextView': 1, 'ImageView': 2, 'Button': 3,
            'EditText': 4, 'WebView': 5, 'View': 6,
            'CheckBox': 7, 'RadioButton': 8, 'Switch': 9,
            'ToggleButton': 10, 'Widget': 11, 'SwitchMain': 12,
            'SwitchSlider': 13
        }

        # ============ 5. å¯è§†åŒ–é¢œè‰²æ˜ å°„ ============
        self.colors = {
            'addition': '#4CAF50',  # ç»¿è‰² - æ–°å¢
            'removal': '#F44336',  # çº¢è‰² - ç§»é™¤
            'movement': '#2196F3',  # è“è‰² - ç§»åŠ¨
            'TextView': '#FF9800',  # æ©™è‰²
            'ImageView': '#3F51B5',  # æ·±è“
            'Button': '#009688',  # é’è‰²
            'EditText': '#9C27B0',  # ç´«è‰²
            'WebView': '#795548',  # æ£•è‰²
            'success': '#4CAF50',  # æˆåŠŸ
            'warning': '#FF9800',  # è­¦å‘Š
            'error': '#F44336',  # é”™è¯¯
        }

        print(f"\n{'=' * 60}")
        print("é›†æˆåˆ†æç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        print(f"è®¾å¤‡: {self.device}")
        print(f"{'=' * 60}")

    def preprocess_image(self, image_path: str) -> torch.Tensor:
        """é¢„å¤„ç†å›¾åƒ"""
        try:
            img = Image.open(image_path).convert('RGB')
            img = img.resize((224, 224))

            # è½¬æ¢ä¸ºtensorå¹¶å½’ä¸€åŒ–
            img_np = np.array(img).astype(np.float32) / 255.0
            img_np = (img_np - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])

            # [H, W, C] -> [C, H, W]
            img_tensor = torch.from_numpy(img_np).permute(2, 0, 1).unsqueeze(0)
            return img_tensor.to(self.device)

        except Exception as e:
            print(f"å›¾åƒé¢„å¤„ç†å¤±è´¥: {e}")
            # è¿”å›éšæœºå›¾åƒä½œä¸ºåå¤‡
            return torch.randn(1, 3, 224, 224).to(self.device)

    def tokenize_text(self, text: str) -> torch.Tensor:
        """ç®€å•æ–‡æœ¬åˆ†è¯"""
        # ç®€å•çš„è¯æ±‡è¡¨æ˜ å°„
        vocab = {
            'Added': 1, 'Removed': 2, 'TextView': 3, 'Button': 4,
            'ImageView': 5, 'position': 6, 'from': 7, 'to': 8,
            'EditText': 9, 'WebView': 10, 'View': 11, 'SwitchMain': 12,
            'SwitchSlider': 13
        }

        # æŒ‰ç©ºæ ¼åˆ†è¯
        tokens = text.split()
        token_ids = []

        for token in tokens[:self.config.max_text_len]:
            if token in vocab:
                token_ids.append(vocab[token])
            elif token.isdigit() or token.replace('.', '').isdigit():
                token_ids.append(14)  # æ•°å­—
            elif token in '();,':  # æ ‡ç‚¹ç¬¦å·
                token_ids.append(15)
            else:
                token_ids.append(16)  # å…¶ä»–

        # å¡«å……
        if len(token_ids) < self.config.max_text_len:
            token_ids.extend([0] * (self.config.max_text_len - len(token_ids)))

        return torch.tensor(token_ids[:self.config.max_text_len], dtype=torch.long).unsqueeze(0).to(self.device)

    def parse_components_from_description(self, description: str) -> Tuple[torch.Tensor, torch.Tensor]:
        """ä»æè¿°ä¸­è§£æç»„ä»¶ä¿¡æ¯"""
        max_components = self.config.max_components

        # åˆå§‹åŒ–ç»„ä»¶çŸ©é˜µ
        ref_components = torch.zeros((1, max_components, 13), dtype=torch.float32)
        tar_components = torch.zeros((1, max_components, 13), dtype=torch.float32)

        ref_idx = 0
        tar_idx = 0

        # è§£æAddedç»„ä»¶ (ç›®æ ‡ç»„ä»¶)
        added_pattern = r'Added (\w+) at position \((\d+),\s*(\d+),\s*(\d+),\s*(\d+)\)'
        for comp_type, x1, y1, x2, y2 in re.findall(added_pattern, description):
            if tar_idx >= max_components:
                break

            # ç±»å‹ç¼–ç 
            type_id = self.component_types.get(comp_type, 6)  # é»˜è®¤View

            # å½’ä¸€åŒ–åæ ‡ (0-1)
            x1_norm = int(x1) / 144.0
            y1_norm = int(y1) / 256.0
            x2_norm = int(x2) / 144.0
            y2_norm = int(y2) / 256.0

            # ç¡®ä¿åœ¨èŒƒå›´å†…
            bbox = [
                max(0.0, min(1.0, x1_norm)),
                max(0.0, min(1.0, y1_norm)),
                max(0.0, min(1.0, x2_norm)),
                max(0.0, min(1.0, y2_norm))
            ]

            # è®¡ç®—ç‰¹å¾
            center_x = (bbox[0] + bbox[2]) / 2
            center_y = (bbox[1] + bbox[3]) / 2
            width = bbox[2] - bbox[0]
            height = bbox[3] - bbox[1]
            area = width * height
            weight = 1.0

            # å¡«å……ç›®æ ‡ç»„ä»¶
            tar_components[0, tar_idx] = torch.tensor([
                float(type_id),
                bbox[0], bbox[1], bbox[2], bbox[3],  # bbox
                center_x, center_y, width, height,  # å‡ ä½•ç‰¹å¾
                area, weight, 0.0, 0.0  # é¢ç§¯ã€æƒé‡ã€éç›®æ ‡ã€æœªå˜åŒ–
            ])
            tar_idx += 1

        # è§£æRemovedç»„ä»¶ (å‚è€ƒç»„ä»¶)
        removed_pattern = r'Removed (\w+) from position \((\d+),\s*(\d+),\s*(\d+),\s*(\d+)\)'
        for comp_type, x1, y1, x2, y2 in re.findall(removed_pattern, description):
            if ref_idx >= max_components:
                break

            type_id = self.component_types.get(comp_type, 6)

            x1_norm = int(x1) / 144.0
            y1_norm = int(y1) / 256.0
            x2_norm = int(x2) / 144.0
            y2_norm = int(y2) / 256.0

            bbox = [
                max(0.0, min(1.0, x1_norm)),
                max(0.0, min(1.0, y1_norm)),
                max(0.0, min(1.0, x2_norm)),
                max(0.0, min(1.0, y2_norm))
            ]

            center_x = (bbox[0] + bbox[2]) / 2
            center_y = (bbox[1] + bbox[3]) / 2
            width = bbox[2] - bbox[0]
            height = bbox[3] - bbox[1]
            area = width * height
            weight = 1.0

            # å¡«å……å‚è€ƒç»„ä»¶
            ref_components[0, ref_idx] = torch.tensor([
                float(type_id),
                bbox[0], bbox[1], bbox[2], bbox[3],
                center_x, center_y, width, height,
                area, weight, 1.0, 0.0  # æ˜¯ç›®æ ‡ã€æœªå˜åŒ–
            ])
            ref_idx += 1

        return ref_components.to(self.device), tar_components.to(self.device)

    def run_stage1_analysis(self, ref_image: torch.Tensor, tar_image: torch.Tensor) -> Dict:
        """è¿è¡Œç¬¬ä¸€é˜¶æ®µåˆ†æ"""
        print("\nç¬¬ä¸€é˜¶æ®µï¼šè§†è§‰åŸºç¡€åˆ†æ...")

        with torch.no_grad():
            outputs = self.stage1_model(ref_image, tar_image)

        # æå–å˜åŒ–mask
        pred_logits = outputs.get('pred_logits')
        if pred_logits is not None:
            change_mask = torch.sigmoid(pred_logits).cpu().numpy()[0]
        else:
            change_mask = np.zeros((224, 224))

        # è®¡ç®—è§†è§‰å·®å¼‚
        diff_map = self.compute_visual_diff(ref_image, tar_image)

        return {
            'diff_features': outputs.get('diff_features'),
            'change_mask': change_mask,
            'change_probability': outputs.get('change_logits', torch.zeros(1, 1)).sigmoid().item(),
            'diff_map': diff_map,
            'raw_outputs': outputs
        }

    def compute_visual_diff(self, ref_image: torch.Tensor, tar_image: torch.Tensor) -> np.ndarray:
        """è®¡ç®—è§†è§‰å·®å¼‚å›¾"""
        ref_np = ref_image.cpu().numpy()[0]
        tar_np = tar_image.cpu().numpy()[0]

        # åå½’ä¸€åŒ–
        mean = np.array([0.485, 0.456, 0.406])[:, None, None]
        std = np.array([0.229, 0.224, 0.225])[:, None, None]

        ref_img = (ref_np * std + mean).transpose(1, 2, 0)
        tar_img = (tar_np * std + mean).transpose(1, 2, 0)

        # è®¡ç®—å·®å¼‚
        diff = np.abs(ref_img - tar_img).mean(axis=2)

        # å¢å¼ºå¯¹æ¯”åº¦
        diff_enhanced = np.power(diff, 0.7)
        if diff_enhanced.max() > 0:
            diff_enhanced = diff_enhanced / diff_enhanced.max()

        return diff_enhanced

    def run_stage2_analysis(self, ref_image: torch.Tensor, tar_image: torch.Tensor,
                            text_tokens: torch.Tensor, ref_components: torch.Tensor,
                            tar_components: torch.Tensor) -> Dict:
        """è¿è¡Œç¬¬äºŒé˜¶æ®µåˆ†æ"""
        print("ç¬¬äºŒé˜¶æ®µï¼šå¤šæ¨¡æ€å¯¹é½åˆ†æ...")

        with torch.no_grad():
            outputs = self.stage2_model(
                ref_image, tar_image, text_tokens,
                ref_components, tar_components
            )

        return {
            'alignment_score': outputs.get('alignment_scores', torch.zeros(1, 1)).item(),
            'alignment_logits': outputs.get('alignment_logits'),
            'fused_features': outputs.get('fused_features'),
            'visual_features': outputs.get('visual_features'),
            'text_features': outputs.get('text_features'),
            'component_features': outputs.get('component_outputs', {}).get('change_features'),
            'raw_outputs': outputs
        }

    def run_stage3_analysis(self, ref_image: torch.Tensor, tar_image: torch.Tensor,
                            text_tokens: torch.Tensor, ref_components: torch.Tensor,
                            tar_components: torch.Tensor, differ_text: str) -> Dict:
        """è¿è¡Œç¬¬ä¸‰é˜¶æ®µåˆ†æ"""
        print("ç¬¬ä¸‰é˜¶æ®µï¼šçŸ­è¯­çº§å¯¹æ¯”åˆ†æ...")

        with torch.no_grad():
            outputs = self.stage3_model(
                ref_image, tar_image, text_tokens,
                ref_components, tar_components, [differ_text]
            )

        return {
            'correspondences': outputs.get('correspondences', []),
            'phrase_features': outputs.get('phrase_features'),
            'patch_features': outputs.get('patch_features'),
            'contrastive_loss': outputs.get('total_contrastive_loss', torch.tensor(0.0)).item(),
            'parsed_phrases': outputs.get('parsed_phrases', []),
            'raw_outputs': outputs
        }

    def analyze_with_models(self, ref_image_path: str, tar_image_path: str,
                            description: str, output_prefix: str = "integrated_analysis") -> Dict:
        """ä½¿ç”¨ä¸‰é˜¶æ®µæ¨¡å‹è¿›è¡Œå®Œæ•´åˆ†æ"""
        print(f"\n{'=' * 60}")
        print("å¼€å§‹é›†æˆGUIåˆ†æ")
        print(f"å‚è€ƒå›¾åƒ: {Path(ref_image_path).name}")
        print(f"ç›®æ ‡å›¾åƒ: {Path(tar_image_path).name}")
        print(f"{'=' * 60}")

        start_time = time.time()

        try:
            # ============ æ•°æ®é¢„å¤„ç† ============
            print("\n1. æ•°æ®é¢„å¤„ç†...")
            ref_image = self.preprocess_image(ref_image_path)
            tar_image = self.preprocess_image(tar_image_path)
            text_tokens = self.tokenize_text(description)
            ref_components, tar_components = self.parse_components_from_description(description)

            # ============ é˜¶æ®µåˆ†æ ============
            print("\n2. æ‰§è¡Œä¸‰é˜¶æ®µåˆ†æ...")

            # ç¬¬ä¸€é˜¶æ®µï¼šè§†è§‰åŸºç¡€åˆ†æ
            stage1_results = self.run_stage1_analysis(ref_image, tar_image)

            # ç¬¬äºŒé˜¶æ®µï¼šå¤šæ¨¡æ€å¯¹é½åˆ†æ
            stage2_results = self.run_stage2_analysis(
                ref_image, tar_image, text_tokens,
                ref_components, tar_components
            )

            # ç¬¬ä¸‰é˜¶æ®µï¼šçŸ­è¯­çº§å¯¹æ¯”åˆ†æ
            stage3_results = self.run_stage3_analysis(
                ref_image, tar_image, text_tokens,
                ref_components, tar_components, description
            )

            # ============ è§£æè¯¦ç»†å˜åŒ– ============
            print("\n3. è§£æè¯¦ç»†å˜åŒ–...")
            detailed_changes = self.parse_detailed_changes(description)

            # ============ ç»¼åˆè¯„ä¼° ============
            print("\n4. ç»¼åˆè¯„ä¼°...")
            overall_assessment = self.assess_overall_quality(
                detailed_changes,
                stage1_results,
                stage2_results,
                stage3_results
            )

            # ============ æ„å»ºç»“æœ ============
            results = {
                'reference_image': ref_image_path,
                'target_image': tar_image_path,
                'description': description,
                'analysis_time': time.time() - start_time,
                'detailed_changes': detailed_changes,
                'stage1_results': stage1_results,
                'stage2_results': stage2_results,
                'stage3_results': stage3_results,
                'overall_assessment': overall_assessment,
                'config': self.config.__dict__
            }

            # ============ ç”Ÿæˆå¯è§†åŒ– ============
            print("\n5. ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š...")
            self.create_integrated_visualization(results, output_prefix)

            # ============ ä¿å­˜ç»“æœ ============
            print("\n6. ä¿å­˜åˆ†æç»“æœ...")
            self.save_analysis_results(results, output_prefix)

            # ============ æ‰“å°æ‘˜è¦ ============
            self.print_summary(results)

            print(f"\n{'=' * 60}")
            print("é›†æˆåˆ†æå®Œæˆ!")
            print(f"{'=' * 60}")

            return results

        except Exception as e:
            print(f"\nâŒ åˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {'error': str(e)}

    def parse_detailed_changes(self, description: str) -> Dict[str, List[Dict]]:
        """è§£æè¯¦ç»†å˜åŒ–"""
        changes = {
            'additions': [],
            'removals': [],
            'movements': [],
            'all_components': []
        }

        # è§£æAdded
        added_pattern = r'Added (\w+) at position \((\d+),\s*(\d+),\s*(\d+),\s*(\d+)\)'
        for comp_type, x1, y1, x2, y2 in re.findall(added_pattern, description):
            changes['additions'].append({
                'type': comp_type,
                'bbox': [int(x1), int(y1), int(x2), int(y2)],
                'center': [(int(x1) + int(x2)) // 2, (int(y1) + int(y2)) // 2],
                'width': int(x2) - int(x1),
                'height': int(y2) - int(y1),
                'area': (int(x2) - int(x1)) * (int(y2) - int(y1)),
                'change_type': 'addition',
                'description': f"Added {comp_type}"
            })

        # è§£æRemoved
        removed_pattern = r'Removed (\w+) from position \((\d+),\s*(\d+),\s*(\d+),\s*(\d+)\)'
        for comp_type, x1, y1, x2, y2 in re.findall(removed_pattern, description):
            changes['removals'].append({
                'type': comp_type,
                'bbox': [int(x1), int(y1), int(x2), int(y2)],
                'center': [(int(x1) + int(x2)) // 2, (int(y1) + int(y2)) // 2],
                'width': int(x2) - int(x1),
                'height': int(y2) - int(y1),
                'area': (int(x2) - int(x1)) * (int(y2) - int(y1)),
                'change_type': 'removal',
                'description': f"Removed {comp_type}"
            })

        # è§£æMoved
        moved_pattern = r'(\w+) from \((\d+),\s*(\d+),\s*(\d+),\s*(\d+)\) to \((\d+),\s*(\d+),\s*(\d+),\s*(\d+)\)'
        for comp_type, fx1, fy1, fx2, fy2, tx1, ty1, tx2, ty2 in re.findall(moved_pattern, description):
            changes['movements'].append({
                'type': comp_type,
                'from_bbox': [int(fx1), int(fy1), int(fx2), int(fy2)],
                'to_bbox': [int(tx1), int(ty1), int(tx2), int(ty2)],
                'from_center': [(int(fx1) + int(fx2)) // 2, (int(fy1) + int(fy2)) // 2],
                'to_center': [(int(tx1) + int(tx2)) // 2, (int(ty1) + int(ty2)) // 2],
                'change_type': 'movement',
                'description': f"Moved {comp_type}"
            })

        # åˆå¹¶æ‰€æœ‰ç»„ä»¶
        changes['all_components'] = (
                changes['additions'] +
                changes['removals'] +
                changes['movements']
        )

        # ç»Ÿè®¡ä¿¡æ¯
        change_types = {}
        component_types = {}
        for comp in changes['all_components']:
            change_type = comp['change_type']
            comp_type = comp['type']
            change_types[change_type] = change_types.get(change_type, 0) + 1
            component_types[comp_type] = component_types.get(comp_type, 0) + 1

        changes['statistics'] = {
            'change_type_distribution': change_types,
            'component_type_distribution': component_types,
            'total_changes': len(changes['all_components'])
        }

        return changes

    def assess_overall_quality(self, detailed_changes: Dict,
                               stage1_results: Dict,
                               stage2_results: Dict,
                               stage3_results: Dict) -> Dict:
        """ç»¼åˆè¯„ä¼°è´¨é‡"""
        stats = detailed_changes['statistics']

        # è§†è§‰å˜åŒ–æ£€æµ‹
        visual_change_prob = stage1_results.get('change_probability', 0)
        visual_change_detected = visual_change_prob > 0.05

        # å¯¹é½åˆ†æ•°
        alignment_score = stage2_results.get('alignment_score', 0)

        # çŸ­è¯­å¯¹é½è´¨é‡
        correspondences = stage3_results.get('correspondences', [])
        phrase_alignment_score = 0.0
        if correspondences:
            phrase_alignment_score = np.mean([c.get('max_score', 0) for c in correspondences])

        # ç»¼åˆç½®ä¿¡åº¦
        if stats['total_changes'] > 0:
            overall_confidence = (
                    visual_change_prob * 0.3 +
                    alignment_score * 0.4 +
                    phrase_alignment_score * 0.3
            )
        else:
            overall_confidence = visual_change_prob if not visual_change_detected else 0.0

        overall_confidence = np.clip(overall_confidence, 0.0, 1.0)

        # éªŒè¯ç»“æœ
        change_validated = (
                visual_change_detected and
                stats['total_changes'] > 0 and
                alignment_score > 0.6 and
                overall_confidence > 0.5
        )

        return {
            'alignment_score': alignment_score,
            'phrase_alignment_score': phrase_alignment_score,
            'visual_change_probability': visual_change_prob,
            'overall_confidence': overall_confidence,
            'described_changes_count': stats['total_changes'],
            'visual_change_detected': visual_change_detected,
            'change_validated': change_validated,
            'summary': self.generate_quality_summary(
                stats, visual_change_detected, change_validated
            )
        }

    def generate_quality_summary(self, stats: Dict, visual_change: bool, validated: bool) -> str:
        """ç”Ÿæˆè´¨é‡æ‘˜è¦"""
        if validated:
            return f"âœ… éªŒè¯é€šè¿‡ï¼š{stats['total_changes']}ä¸ªå˜åŒ–ä¸è§†è§‰æ£€æµ‹ä¸€è‡´"

        if stats['total_changes'] == 0 and not visual_change:
            return "âœ… æ— å˜åŒ–æ£€æµ‹"

        if stats['total_changes'] == 0 and visual_change:
            return "âš ï¸ æœ‰è§†è§‰å˜åŒ–ä½†æ— æè¿°"

        if stats['total_changes'] > 0 and not visual_change:
            return "âš ï¸ æœ‰æè¿°å˜åŒ–ä½†æ— æ˜¾è‘—è§†è§‰å˜åŒ–"

        return "â“ å˜åŒ–éœ€è¦è¿›ä¸€æ­¥éªŒè¯"

    def create_integrated_visualization(self, results: Dict, output_prefix: str):
        """åˆ›å»ºé›†æˆå¯è§†åŒ–æŠ¥å‘Š"""
        try:
            fig = plt.figure(figsize=(28, 24))

            # åŠ è½½å›¾åƒ
            ref_img = Image.open(results['reference_image']).convert('RGB').resize((340, 340))
            tar_img = Image.open(results['target_image']).convert('RGB').resize((340, 340))

            # è®¾ç½®ç½‘æ ¼
            gs = plt.GridSpec(3, 3, hspace=0.4, wspace=0.4)

            # 1. å‚è€ƒå›¾åƒ
            ax1 = plt.subplot(gs[0, 0])
            ax1.imshow(ref_img)
            ax1.set_title('Reference Image\n(Original GUI)', fontsize=24, fontweight='bold', pad=20)
            ax1.axis('off')

            # æ ‡æ³¨ç§»é™¤ç»„ä»¶
            for removal in results['detailed_changes']['removals'][:2]:
                bbox = removal['bbox']
                # ç¼©æ”¾åæ ‡åˆ°æ˜¾ç¤ºå°ºå¯¸
                scale_x = 340 / 144
                scale_y = 340 / 256
                scaled_bbox = [bbox[0] * scale_x, bbox[1] * scale_y, bbox[2] * scale_x, bbox[3] * scale_y]

                rect = patches.Rectangle(
                    (scaled_bbox[0], scaled_bbox[1]),
                    scaled_bbox[2] - scaled_bbox[0], scaled_bbox[3] - scaled_bbox[1],
                    linewidth=3, edgecolor=self.colors['removal'],
                    facecolor='none', linestyle='--', alpha=0.8
                )
                ax1.add_patch(rect)

            # 2. ç›®æ ‡å›¾åƒ
            ax2 = plt.subplot(gs[0, 1])
            ax2.imshow(tar_img)
            ax2.set_title('Target Image\n(Modified GUI)', fontsize=24, fontweight='bold', pad=20)
            ax2.axis('off')

            # æ ‡æ³¨æ–°å¢ç»„ä»¶
            for addition in results['detailed_changes']['additions'][:2]:
                bbox = addition['bbox']
                scale_x = 340 / 144
                scale_y = 340 / 256
                scaled_bbox = [bbox[0] * scale_x, bbox[1] * scale_y, bbox[2] * scale_x, bbox[3] * scale_y]

                rect = patches.Rectangle(
                    (scaled_bbox[0], scaled_bbox[1]),
                    scaled_bbox[2] - scaled_bbox[0], scaled_bbox[3] - scaled_bbox[1],
                    linewidth=3, edgecolor=self.colors['addition'],
                    facecolor='none', linestyle='-', alpha=0.8
                )
                ax2.add_patch(rect)

            # 3. çƒ­åŠ›å›¾
            ax3 = plt.subplot(gs[0, 2])
            diff_map = results['stage1_results'].get('diff_map')
            if diff_map is not None:
                im = ax3.imshow(diff_map, cmap='hot', vmin=0, vmax=1)
                ax3.set_title('Stage1: Visual Change Heatmap', fontsize=24, fontweight='bold', pad=20)
                plt.colorbar(im, ax=ax3, fraction=0.046, pad=0.04)
            else:
                ax3.text(0.5, 0.5, 'No heatmap data', ha='center', va='center', fontsize=20)
                ax3.set_title('Stage1: Visual Change Heatmap', fontsize=24, fontweight='bold', pad=20)

            # 4. Stage1å˜åŒ–mask
            ax4 = plt.subplot(gs[1, 0])
            change_mask = results['stage1_results'].get('change_mask')
            if change_mask is not None:
                ax4.imshow(change_mask, cmap='binary')
                ax4.set_title('Stage1: Change Detection Mask', fontsize=24, fontweight='bold', pad=20)
            else:
                ax4.text(0.5, 0.5, 'No mask data', ha='center', va='center', fontsize=20)
                ax4.set_title('Stage1: Change Detection Mask', fontsize=24, fontweight='bold', pad=20)

            # 5. Stage2å¯¹é½ç»“æœ
            ax5 = plt.subplot(gs[1, 1])
            ax5.axis('off')

            alignment_text = "Stage2: Multi-modal Alignment\n"
            alignment_text += "=" * 30 + "\n\n"

            alignment_score = results['stage2_results'].get('alignment_score', 0)
            alignment_text += f"Alignment Score: {alignment_score:.4f}\n\n"

            visual_feat_norm = torch.norm(results['stage2_results'].get('visual_features', torch.zeros(1, 1))).item()
            text_feat_norm = torch.norm(results['stage2_results'].get('text_features', torch.zeros(1, 1))).item()

            alignment_text += f"Visual Feature Norm: {visual_feat_norm:.3f}\n"
            alignment_text += f"Text Feature Norm: {text_feat_norm:.3f}\n"
            alignment_text += f"Similarity: {alignment_score:.3f}"

            ax5.text(0.1, 0.95, alignment_text, fontsize=20,
                     verticalalignment='top', transform=ax5.transAxes,
                     bbox=dict(boxstyle='round', facecolor='#f0f0f0', alpha=0.9))

            # 6. Stage3çŸ­è¯­å¯¹é½
            ax6 = plt.subplot(gs[1, 2])
            ax6.axis('off')

            phrase_text = "Stage3: Phrase-Patch Alignment\n"
            phrase_text += "=" * 30 + "\n\n"

            correspondences = results['stage3_results'].get('correspondences', [])
            phrase_text += f"Phrases found: {len(correspondences)}\n\n"

            for i, corr in enumerate(correspondences[:3]):
                phrase_text += f"Phrase {i + 1}:\n"
                phrase_text += f"  Max score: {corr.get('max_score', 0):.3f}\n"
                phrase_text += f"  Top patches: {len(corr.get('top_patches', []))}\n\n"

            if correspondences:
                avg_score = np.mean([c.get('max_score', 0) for c in correspondences])
                phrase_text += f"Avg match score: {avg_score:.3f}"

            ax6.text(0.1, 0.95, phrase_text, fontsize=20,
                     verticalalignment='top', transform=ax6.transAxes,
                     bbox=dict(boxstyle='round', facecolor='#f0f0f0', alpha=0.9))

            # 7. è¯¦ç»†å˜åŒ–åˆ—è¡¨
            ax7 = plt.subplot(gs[2, 0])
            ax7.axis('off')

            changes_text = "ğŸ” Detailed Changes\n"
            changes_text += "=" * 20 + "\n\n"

            changes = results['detailed_changes']
            changes_text += f"Total: {changes['statistics']['total_changes']}\n"
            changes_text += f"Added: {len(changes['additions'])}\n"
            changes_text += f"Removed: {len(changes['removals'])}\n"
            changes_text += f"Moved: {len(changes['movements'])}\n\n"

            # æ˜¾ç¤ºå‰3ä¸ªå˜åŒ–
            for i, comp in enumerate(changes['all_components'][:3]):
                changes_text += f"{i + 1}. {comp['description']}\n"

            ax7.text(0.1, 0.95, changes_text, fontsize=20,
                     verticalalignment='top', transform=ax7.transAxes,
                     bbox=dict(boxstyle='round', facecolor='#f0f0f0', alpha=0.9))

            # 8. ç»¼åˆè¯„ä¼°
            ax8 = plt.subplot(gs[2, 1])
            ax8.axis('off')

            assessment = results['overall_assessment']
            assessment_text = "âœ… Overall Assessment\n"
            assessment_text += "=" * 20 + "\n\n"

            assessment_text += f"Alignment: {assessment['alignment_score']:.2%}\n"
            assessment_text += f"Phrase Alignment: {assessment['phrase_alignment_score']:.2%}\n"
            assessment_text += f"Overall Confidence: {assessment['overall_confidence']:.2%}\n\n"

            assessment_text += f"Validation: {'âœ… PASS' if assessment['change_validated'] else 'âŒ FAIL'}\n\n"
            assessment_text += f"Summary:\n{assessment['summary']}"

            color = self.colors['success'] if assessment['change_validated'] else self.colors['warning']
            ax8.text(0.1, 0.95, assessment_text, fontsize=20,
                     verticalalignment='top', transform=ax8.transAxes,
                     bbox=dict(boxstyle='round', facecolor=color, alpha=0.2, edgecolor=color, linewidth=3))

            # 9. æ¨¡å‹ä¿¡æ¯
            ax9 = plt.subplot(gs[2, 2])
            ax9.axis('off')

            model_text = "ğŸ¤– Model Information\n"
            model_text += "=" * 20 + "\n\n"
            model_text += f"Stage1: Visual Model\n"
            model_text += f"Stage2: Multi-modal Model\n"
            model_text += f"Stage3: Phrase Model\n\n"
            model_text += f"Analysis Time: {results['analysis_time']:.1f}s\n"
            model_text += f"Device: {self.device}\n\n"
            model_text += f"Total Parameters:\n"
            model_text += f"  Stage1: {sum(p.numel() for p in self.stage1_model.parameters()):,}\n"
            model_text += f"  Stage2: {sum(p.numel() for p in self.stage2_model.parameters()):,}\n"
            model_text += f"  Stage3: {sum(p.numel() for p in self.stage3_model.parameters()):,}"

            ax9.text(0.1, 0.95, model_text, fontsize=18,
                     verticalalignment='top', transform=ax9.transAxes,
                     bbox=dict(boxstyle='round', facecolor='#f0f0f0', alpha=0.9))

            # ä¸»æ ‡é¢˜
            fig.suptitle('Three-Stage GUI Change Analysis - Integrated Report',
                         fontsize=32, fontweight='bold', y=0.98)

            # å‰¯æ ‡é¢˜
            ref_name = Path(results['reference_image']).name
            tar_name = Path(results['target_image']).name
            fig.text(0.5, 0.95,
                     f"Reference: {ref_name}  |  Target: {tar_name}",
                     fontsize=20, ha='center', style='italic')

            # ä¿å­˜
            output_path = f"{output_prefix}_integrated_report.png"
            plt.tight_layout(rect=[0, 0.03, 1, 0.95])
            plt.savefig(output_path, dpi=150, bbox_inches='tight')
            plt.close()

            print(f"âœ… é›†æˆå¯è§†åŒ–æŠ¥å‘Šä¿å­˜: {output_path}")

        except Exception as e:
            print(f"âŒ å¯è§†åŒ–åˆ›å»ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

    def save_analysis_results(self, results: Dict, output_prefix: str):
        """ä¿å­˜åˆ†æç»“æœ"""
        try:
            # åˆ›å»ºå¯åºåˆ—åŒ–çš„å‰¯æœ¬
            def make_serializable(obj):
                if isinstance(obj, (str, int, float, bool, type(None))):
                    return obj
                elif isinstance(obj, (list, tuple)):
                    return [make_serializable(item) for item in obj]
                elif isinstance(obj, dict):
                    return {k: make_serializable(v) for k, v in obj.items()}
                elif isinstance(obj, torch.Tensor):
                    return obj.cpu().numpy().tolist()
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, np.generic):
                    return obj.item()
                else:
                    return str(obj)

            serializable_results = make_serializable(results)

            # ä¿å­˜JSON
            json_path = f"{output_prefix}_results.json"
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(serializable_results, f, indent=2, ensure_ascii=False)

            print(f"âœ… åˆ†æç»“æœä¿å­˜: {json_path}")

        except Exception as e:
            print(f"âŒ ç»“æœä¿å­˜å¤±è´¥: {e}")

    def print_summary(self, results: Dict):
        """æ‰“å°åˆ†ææ‘˜è¦"""
        print(f"\n{'=' * 60}")
        print("åˆ†ææ‘˜è¦")
        print(f"{'=' * 60}")

        # åŸºç¡€ä¿¡æ¯
        print(f"ğŸ“ å›¾åƒ:")
        print(f"  å‚è€ƒ: {Path(results['reference_image']).name}")
        print(f"  ç›®æ ‡: {Path(results['target_image']).name}")
        print(f"  åˆ†ææ—¶é—´: {results['analysis_time']:.2f}ç§’")

        # å˜åŒ–ç»Ÿè®¡
        changes = results['detailed_changes']['statistics']
        print(f"\nğŸ“Š å˜åŒ–ç»Ÿè®¡:")
        print(f"  æ€»å˜åŒ–: {changes['total_changes']}")
        print(f"  å˜åŒ–ç±»å‹: {changes['change_type_distribution']}")
        print(f"  ç»„ä»¶ç±»å‹: {changes['component_type_distribution']}")

        # é˜¶æ®µç»“æœ
        print(f"\nğŸš€ é˜¶æ®µç»“æœ:")
        stage1_prob = results['stage1_results'].get('change_probability', 0)
        print(f"  Stage1 - è§†è§‰å˜åŒ–æ¦‚ç‡: {stage1_prob:.2%}")

        stage2_score = results['stage2_results'].get('alignment_score', 0)
        print(f"  Stage2 - å¯¹é½åˆ†æ•°: {stage2_score:.2%}")

        correspondences = results['stage3_results'].get('correspondences', [])
        print(f"  Stage3 - çŸ­è¯­å¯¹é½æ•°é‡: {len(correspondences)}")

        # ç»¼åˆè¯„ä¼°
        assessment = results['overall_assessment']
        print(f"\nâœ… ç»¼åˆè¯„ä¼°:")
        print(f"  å¯¹é½åˆ†æ•°: {assessment['alignment_score']:.2%}")
        print(f"  çŸ­è¯­å¯¹é½: {assessment['phrase_alignment_score']:.2%}")
        print(f"  æ•´ä½“ç½®ä¿¡åº¦: {assessment['overall_confidence']:.2%}")
        print(f"  éªŒè¯ç»“æœ: {'âœ… PASS' if assessment['change_validated'] else 'âŒ FAIL'}")
        print(f"  æ€»ç»“: {assessment['summary']}")


# ============ é…ç½®ç±» ============
class Config:
    """é…ç½®æ–‡ä»¶"""

    def __init__(self):
        self.hidden_dim = 768
        self.max_text_len = 512
        self.max_components = 20
        self.visual_dim = 768
        self.image_model = "vit-large-patch16-224"
        self.model_root = "/home/common-dir/models"
        self.output_dir = "./integrated_analysis_results"
        self.learning_rate = 1e-4
        self.batch_size = 1  # æ¨ç†æ—¶ä½¿ç”¨batch size 1
        self.num_workers = 0
        self.pin_memory = False
        self.cuda_available = torch.cuda.is_available()
        self.device = torch.device('cuda' if self.cuda_available else 'cpu')
        self.mixed_precision = False
        self.gradient_checkpointing = False


# ============ ä¸»å‡½æ•° ============
def main():
    """ä¸»å‡½æ•°"""
    # åŠ è½½é…ç½®
    config = Config()

    # åˆ›å»ºåˆ†æå™¨
    analyzer = IntegratedGUIAnalyzer(config)

    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            'name': 'Login Screen Analysis',
            'ref_image': "/home/common-dir/data/gui/settings/23319.png",
            'tar_image': "/home/common-dir/data/gui/settings/57622.png",
            'description': """Added TextView at position (0, 83, 144, 188); Added View at position (0, 9, 144, 38); Added SwitchMain at position (0, 37, 72, 81); Added SwitchSlider at position (72, 37, 144, 81); TextView from (26, 130, 40, 137) to (0, 83, 144, 188); TextView from (26, 111, 49, 118) to (0, 83, 144, 188)"""
        }
    ]

    for i, test in enumerate(test_cases):
        print(f"\n{'=' * 60}")
        print(f"æµ‹è¯•ç”¨ä¾‹ {i + 1}: {test['name']}")
        print(f"{'=' * 60}")

        # è¿è¡Œé›†æˆåˆ†æ
        results = analyzer.analyze_with_models(
            test['ref_image'],
            test['tar_image'],
            test['description'],
            output_prefix=f"test_case_{i + 1}"
        )

        if 'error' not in results:
            print(f"\nâœ… åˆ†æå®Œæˆ!")
            print(f"  å¯è§†åŒ–æŠ¥å‘Šå·²ä¿å­˜")
            print(f"  è¯¦ç»†ç»“æœå·²ä¿å­˜")
        else:
            print(f"\nâŒ åˆ†æå¤±è´¥: {results['error']}")


if __name__ == "__main__":
    main()
