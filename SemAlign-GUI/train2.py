#!/usr/bin/env python3
"""
ç¬¬äºŒé˜¶æ®µè®­ç»ƒï¼šç»„ä»¶æ„ŸçŸ¥çš„è§†è§‰-æ–‡æœ¬å¯¹é½æ¨¡å‹ - æ­£å¼ç‰ˆæœ¬
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from pathlib import Path
import numpy as np
import time
import os
import sys
from tqdm import tqdm

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config import config
from dataloder import create_data_loader
from model2 import Stage2AlignmentModel
from memory import memory_monitor


class Stage2AlignmentTrainer:
    """ç¬¬äºŒé˜¶æ®µå¯¹é½è®­ç»ƒå™¨"""

    def __init__(self, stage1_checkpoint: str, use_wandb: bool = True):
        self.config = config
        self.device = config.device
        self.use_wandb = use_wandb

        print(f"\n{'=' * 60}")
        print("ç¬¬äºŒé˜¶æ®µè®­ç»ƒï¼šè§†è§‰-æ–‡æœ¬-ç»„ä»¶å¯¹é½æ¨¡å‹")
        print(f"{'=' * 60}")
        print(f"è®¾å¤‡: {self.device}")
        print(f"éšè—ç»´åº¦: {self.config.hidden_dim}")
        print(f"æ‰¹æ¬¡å¤§å°: {self.config.batch_size}")

        # æ£€æŸ¥æ£€æŸ¥ç‚¹
        if not Path(stage1_checkpoint).exists():
            print(f"è­¦å‘Šï¼šæ‰¾ä¸åˆ°stage1æ£€æŸ¥ç‚¹ {stage1_checkpoint}")
            print("å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„è§†è§‰ç¼–ç å™¨")

        self.stage1_checkpoint = stage1_checkpoint

        # ============ åˆå§‹åŒ–æ¨¡å‹ ============
        print("\nåˆå§‹åŒ–ç»„ä»¶æ„ŸçŸ¥å¯¹é½æ¨¡å‹...")
        memory_monitor.print_memory_stats("åˆå§‹åŒ–å‰")

        self.model = Stage2AlignmentModel(
            stage1_checkpoint=self.stage1_checkpoint,
            config=self.config,
            use_components=True
        ).to(self.device)

        # æ‰“å°å‚æ•°ç»Ÿè®¡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"æ€»å‚æ•°: {total_params:,}")
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"å†»ç»“æ¯”ä¾‹: {(total_params - trainable_params) / total_params * 100:.1f}%")

        # ============ ä¼˜åŒ–å™¨é…ç½® ============
        # åˆ†å±‚å­¦ä¹ ç‡
        text_params = []
        component_params = []
        fusion_params = []
        other_params = []

        for name, param in self.model.named_parameters():
            if not param.requires_grad:
                continue

            if 'text_encoder' in name:
                text_params.append(param)
            elif 'component_encoder' in name:
                component_params.append(param)
            elif 'fusion_module' in name or 'alignment_head' in name or 'contrastive_proj' in name:
                fusion_params.append(param)
            else:
                other_params.append(param)

        print(f"\nä¼˜åŒ–å™¨å‚æ•°åˆ†ç»„:")
        print(f"  æ–‡æœ¬å‚æ•°: {len(text_params)}å±‚")
        print(f"  ç»„ä»¶å‚æ•°: {len(component_params)}å±‚")
        print(f"  èåˆå‚æ•°: {len(fusion_params)}å±‚")
        print(f"  å…¶ä»–å‚æ•°: {len(other_params)}å±‚")

        # ä¼˜åŒ–å™¨é…ç½®
        self.optimizer = optim.AdamW([
            {'params': text_params, 'lr': 1e-4, 'weight_decay': 0.01},
            {'params': component_params, 'lr': 5e-4, 'weight_decay': 0.001},
            {'params': fusion_params, 'lr': 3e-4, 'weight_decay': 0.001},
            {'params': other_params, 'lr': 3e-4, 'weight_decay': 0.01}
        ])

        # ============ æŸå¤±å‡½æ•°æƒé‡ ============
        self.base_loss_weights = {
            'alignment': 0.6,
            'visual_text': 0.8,
            'contrastive': 0.2,
            'comp_visual': 0.6,
            'comp_text': 0.4,
            'change_type': 0.2,
            'gate_entropy': 0.05
        }

        self.loss_weights = self.base_loss_weights.copy()

        print(f"\nåˆå§‹æŸå¤±æƒé‡é…ç½®:")
        for key, weight in self.loss_weights.items():
            print(f"  {key}: {weight}")

        # ============ æ•°æ®åŠ è½½å™¨ ============
        self.train_loader = create_data_loader('train', self.config, is_stage1=False)
        self.val_loader = create_data_loader('val', self.config, is_stage1=False)

        total_steps = self.config.stage2_epochs * len(self.train_loader)

        # ============ å­¦ä¹ ç‡è°ƒåº¦å™¨ ============
        self.scheduler = optim.lr_scheduler.OneCycleLR(
            self.optimizer,
            max_lr=[1e-4, 5e-4, 3e-4, 3e-4],
            total_steps=total_steps,
            pct_start=0.1,
            anneal_strategy='cos'
        )

        print(f"\næ•°æ®é›†ç»Ÿè®¡:")
        print(f"  è®­ç»ƒé›†: {len(self.train_loader.dataset)} æ ·æœ¬")
        print(f"  éªŒè¯é›†: {len(self.val_loader.dataset)} æ ·æœ¬")
        print(f"  æ€»è®­ç»ƒæ­¥æ•°: {total_steps}")

        # ============ æŸå¤±å‡½æ•° ============
        self.alignment_loss_fn = nn.BCEWithLogitsLoss(reduction='mean')
        self.contrastive_loss_fn = nn.CrossEntropyLoss(reduction='mean')
        self.change_type_loss_fn = nn.CrossEntropyLoss(reduction='mean')

        # ============ è®­ç»ƒç›‘æ§ ============
        self.loss_history = {}
        self.similarity_history = {
            'visual_text': [],
            'comp_visual': [],
            'comp_text': []
        }
        self.gradient_history = {
            'text': [],
            'component': [],
            'fusion': []
        }

        # ============ è¾“å‡ºç›®å½• ============
        self.stage_dir = self.config.output_dir / "stage2_alignment"
        self.checkpoint_dir = self.stage_dir / "checkpoints"
        self.log_dir = self.stage_dir / "logs"

        # åˆ›å»ºç›®å½•
        self.stage_dir.mkdir(parents=True, exist_ok=True)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.log_dir.mkdir(parents=True, exist_ok=True)

        # ============ è®­ç»ƒçŠ¶æ€ ============
        self.global_step = 0
        self.current_epoch = 0
        self.best_val_loss = float('inf')
        self.best_alignment_score = 0.0
        self.patience_counter = 0
        self.max_patience = 8

        memory_monitor.print_memory_stats("åˆå§‹åŒ–å")

    def compute_alignment_loss(self, outputs, batch):
        """è®¡ç®—å¯¹é½æŸå¤±"""
        losses = {}

        try:
            batch_size = outputs['visual_features'].shape[0]

            # æå–ç‰¹å¾å¹¶å½’ä¸€åŒ–
            visual_features = F.normalize(outputs['visual_features'], dim=1)
            text_features = F.normalize(outputs['text_features'], dim=1)

            outputs['visual_features'] = visual_features
            outputs['text_features'] = text_features

            # 1. å¯¹é½é¢„æµ‹æŸå¤±
            if 'alignment_logits' in outputs:
                alignment_logits = outputs['alignment_logits']
            else:
                alignment_scores = outputs['alignment_scores']
                alignment_logits = torch.log(alignment_scores / (1 - alignment_scores + 1e-8))

            if 'has_change' in batch:
                alignment_labels = batch['has_change'].unsqueeze(1).float()
                alignment_labels = alignment_labels.to(alignment_logits.device)
                alignment_loss = self.alignment_loss_fn(alignment_logits, alignment_labels)
                losses['alignment'] = alignment_loss * self.loss_weights['alignment']
            else:
                losses['alignment'] = torch.tensor(0.0, device=visual_features.device)

            # 2. è§†è§‰-æ–‡æœ¬å¯¹é½æŸå¤±
            similarity = F.cosine_similarity(visual_features, text_features, dim=1)
            v_t_similarity = similarity.mean()
            v_t_loss = 1.0 - v_t_similarity
            losses['visual_text'] = v_t_loss * self.loss_weights['visual_text']

            # è®°å½•ç›¸ä¼¼åº¦
            if self.global_step % 50 == 0:
                self.similarity_history['visual_text'].append(v_t_similarity.item())

            # 3. å¯¹æ¯”å­¦ä¹ æŸå¤±
            contrastive_features = F.normalize(outputs['contrastive_features'], dim=-1)
            sim_matrix = torch.matmul(contrastive_features, contrastive_features.T)

            temperature = outputs.get('temperature', torch.tensor(0.07, device=contrastive_features.device))
            sim_matrix = sim_matrix / temperature

            labels = torch.arange(batch_size, device=contrastive_features.device)
            contrastive_loss = self.contrastive_loss_fn(sim_matrix, labels)
            losses['contrastive'] = contrastive_loss * self.loss_weights['contrastive']

            # 4. ç»„ä»¶ç›¸å…³æŸå¤±
            if 'component_outputs' in outputs:
                component_features = F.normalize(outputs['component_outputs']['change_features'], dim=1)

                # ç»„ä»¶-è§†è§‰å¯¹é½
                comp_vis_sim = F.cosine_similarity(component_features, visual_features, dim=1)
                comp_vis_similarity = comp_vis_sim.mean()
                losses['comp_visual'] = (1.0 - comp_vis_similarity) * self.loss_weights['comp_visual']

                if self.global_step % 50 == 0:
                    self.similarity_history['comp_visual'].append(comp_vis_similarity.item())

                # ç»„ä»¶-æ–‡æœ¬å¯¹é½
                comp_text_sim = F.cosine_similarity(component_features, text_features, dim=1)
                comp_text_similarity = comp_text_sim.mean()
                losses['comp_text'] = (1.0 - comp_text_similarity) * self.loss_weights['comp_text']

                if self.global_step % 50 == 0:
                    self.similarity_history['comp_text'].append(comp_text_similarity.item())

                # å˜åŒ–ç±»å‹åˆ†ç±»
                if 'change_type_logits' in outputs['component_outputs']:
                    change_type_logits = outputs['component_outputs']['change_type_logits']
                    if 'change_type' in batch:
                        change_type = batch['change_type']
                        change_type = change_type.to(change_type_logits.device)
                        if change_type.dim() > 1 and change_type.shape[-1] > 1:
                            change_type = change_type.argmax(dim=1)
                        change_type_loss = self.change_type_loss_fn(change_type_logits, change_type)
                        losses['change_type'] = change_type_loss * self.loss_weights['change_type']
                    else:
                        losses['change_type'] = torch.tensor(0.0, device=change_type_logits.device)

            # 5. å¤šæ¨¡æ€ä¸€è‡´æ€§æŸå¤±
            if 'fusion_outputs' in outputs:
                fusion_out = outputs['fusion_outputs']
                if 'gate_values' in fusion_out:
                    gate_values = fusion_out['gate_values']
                    entropy_loss = -torch.sum(gate_values * torch.log(gate_values + 1e-8), dim=1).mean()
                    losses['gate_entropy'] = -0.1 * entropy_loss * self.loss_weights['gate_entropy']

            # æ€»æŸå¤±
            total_loss = sum(losses.values())
            losses['total'] = total_loss

            # è®°å½•æŸå¤±å†å²
            self.record_loss_history(losses)

            # åŠ¨æ€è°ƒæ•´æŸå¤±æƒé‡
            if self.global_step % 100 == 0:
                self.dynamically_adjust_weights()

        except Exception as e:
            print(f"æŸå¤±è®¡ç®—é”™è¯¯: {e}")
            device = outputs.get('visual_features', torch.zeros(1).to(self.device)).device
            total_loss = torch.tensor(1.0, device=device, requires_grad=True)
            losses = {'total': total_loss}

        return total_loss, losses

    def record_loss_history(self, losses):
        """è®°å½•æŸå¤±å†å²"""
        for key, value in losses.items():
            if key not in self.loss_history:
                self.loss_history[key] = []

            if torch.is_tensor(value):
                self.loss_history[key].append(value.item())
            else:
                self.loss_history[key].append(value)

    def dynamically_adjust_weights(self):
        """åŠ¨æ€è°ƒæ•´æŸå¤±æƒé‡"""
        if self.global_step == 0:
            return

        weights_changed = False
        for sim_type in ['visual_text', 'comp_visual', 'comp_text']:
            if len(self.similarity_history[sim_type]) >= 5:
                recent_sim = np.mean(self.similarity_history[sim_type][-5:])

                # è°ƒæ•´æƒé‡
                if recent_sim < 0.3:
                    if sim_type == 'visual_text':
                        new_weight = min(1.0, self.base_loss_weights['visual_text'] * 1.1)
                        if abs(new_weight - self.loss_weights['visual_text']) > 0.01:
                            self.loss_weights['visual_text'] = new_weight
                            weights_changed = True
                    elif sim_type == 'comp_visual':
                        new_weight = min(1.0, self.base_loss_weights['comp_visual'] * 1.1)
                        if abs(new_weight - self.loss_weights['comp_visual']) > 0.01:
                            self.loss_weights['comp_visual'] = new_weight
                            weights_changed = True

                elif recent_sim > 0.7:
                    if sim_type == 'visual_text':
                        new_weight = max(0.1, self.base_loss_weights['visual_text'] * 0.9)
                        if abs(new_weight - self.loss_weights['visual_text']) > 0.01:
                            self.loss_weights['visual_text'] = new_weight
                            weights_changed = True
                    elif sim_type == 'comp_visual':
                        new_weight = max(0.1, self.base_loss_weights['comp_visual'] * 0.9)
                        if abs(new_weight - self.loss_weights['comp_visual']) > 0.01:
                            self.loss_weights['comp_visual'] = new_weight
                            weights_changed = True

        if weights_changed and self.global_step % 500 == 0:
            print(f"è°ƒæ•´æŸå¤±æƒé‡ (Step {self.global_step}):")
            for key in ['visual_text', 'comp_visual', 'comp_text']:
                print(f"  {key}: {self.loss_weights[key]:.3f}")

    def monitor_gradients(self):
        """ç›‘æ§æ¢¯åº¦"""
        grad_stats = {}
        for name, param in self.model.named_parameters():
            if param.grad is not None and param.requires_grad:
                grad_norm = param.grad.norm().item()
                if 'text_encoder' in name:
                    grad_stats.setdefault('text', []).append(grad_norm)
                elif 'component_encoder' in name:
                    grad_stats.setdefault('component', []).append(grad_norm)
                elif 'fusion' in name or 'alignment' in name or 'contrastive' in name:
                    grad_stats.setdefault('fusion', []).append(grad_norm)

        # è®°å½•æ¢¯åº¦å†å²
        for module, norms in grad_stats.items():
            if norms:
                mean_grad = np.mean(norms)
                self.gradient_history[module].append(mean_grad)

        # å®šæœŸæ‰“å°æ¢¯åº¦ç»Ÿè®¡
        if self.global_step % 200 == 0 and self.global_step > 0:
            print(f"\næ¢¯åº¦ç»Ÿè®¡ (Step {self.global_step}):")
            for module in ['text', 'component', 'fusion']:
                if self.gradient_history[module]:
                    recent = self.gradient_history[module][-20:] if len(self.gradient_history[module]) >= 20 else \
                        self.gradient_history[module]
                    if recent:
                        avg_grad = np.mean(recent)
                        print(f"  {module}: {avg_grad:.6f}")

    def train_epoch(self, epoch: int):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        epoch_losses = {}

        # é¢„çƒ­ç­–ç•¥
        if epoch < 2:
            self.loss_weights['visual_text'] = 0.5
            self.loss_weights['comp_visual'] = 0.5
            if epoch == 0:
                print(f"é¢„çƒ­é˜¶æ®µ {epoch + 1}/2")
        elif epoch == 2:
            self.loss_weights = self.base_loss_weights.copy()
            print(f"é¢„çƒ­ç»“æŸï¼Œä½¿ç”¨å®Œæ•´æŸå¤±æƒé‡")

        train_loader = self.train_loader
        pbar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{self.config.stage2_epochs}")

        for batch_idx, batch in enumerate(pbar):
            if batch is None:
                continue

            try:
                # å‡†å¤‡æ•°æ®
                ref_image = batch['ref_image'].to(self.device, non_blocking=True)
                tar_image = batch['tar_image'].to(self.device, non_blocking=True)
                text_tokens = batch['text_tokens'].to(self.device, non_blocking=True)
                ref_components = batch['ref_components'].to(self.device, non_blocking=True)
                tar_components = batch['tar_components'].to(self.device, non_blocking=True)

                # ç¡®ä¿ç»„ä»¶æ•°æ®æœ‰æ•ˆ
                if ref_components is not None:
                    ref_components[:, :, 0] = torch.clamp(ref_components[:, :, 0].long(), 0, 19)
                if tar_components is not None:
                    tar_components[:, :, 0] = torch.clamp(tar_components[:, :, 0].long(), 0, 19)

                # æ¸…é›¶æ¢¯åº¦
                self.optimizer.zero_grad(set_to_none=True)

                # å‰å‘ä¼ æ’­
                outputs = self.model(ref_image, tar_image, text_tokens,
                                     ref_components, tar_components)

                total_loss, loss_dict = self.compute_alignment_loss(outputs, batch)

                # æ£€æŸ¥æŸå¤±
                if not total_loss.requires_grad:
                    continue

                if torch.isnan(total_loss) or torch.isinf(total_loss):
                    continue

                # åå‘ä¼ æ’­
                total_loss.backward()

                # æ¢¯åº¦ç´¯ç§¯
                if (batch_idx + 1) % self.config.grad_accum_steps == 0:
                    # æ¢¯åº¦ç›‘æ§
                    if self.global_step % 100 == 0:
                        self.monitor_gradients()

                    # æ¢¯åº¦è£å‰ª
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                    # ä¼˜åŒ–å™¨æ›´æ–°
                    self.optimizer.step()
                    self.optimizer.zero_grad(set_to_none=True)
                    self.scheduler.step()
                    self.global_step += 1

                # ç´¯ç§¯æŸå¤±
                for key, value in loss_dict.items():
                    if key not in epoch_losses:
                        epoch_losses[key] = []
                    epoch_losses[key].append(value.item())

                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'loss': f"{total_loss.item():.4f}",
                    'v_t': f"{loss_dict.get('visual_text', 0):.4f}",
                    'c_v': f"{loss_dict.get('comp_visual', 0):.4f}",
                    'step': self.global_step
                })

                # å®šæœŸæ¸…ç†ç¼“å­˜
                if batch_idx % 50 == 0:
                    memory_monitor.clear_cache()
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()

            except Exception as e:
                if batch_idx == 0:
                    print(f"æ‰¹æ¬¡ {batch_idx} é”™è¯¯: {e}")
                self.optimizer.zero_grad(set_to_none=True)
                continue

        # è®¡ç®—å¹³å‡æŸå¤±
        avg_losses = {}
        for key, values in epoch_losses.items():
            if values:
                avg_losses[key] = np.mean(values)
            else:
                avg_losses[key] = 0.0

        return avg_losses

    @torch.no_grad()
    def validate(self):
        """éªŒè¯è¿‡ç¨‹"""
        self.model.eval()
        val_losses = []
        all_metrics = []
        similarity_stats = {
            'visual_text': [],
            'comp_visual': [],
            'comp_text': []
        }

        print(f"\néªŒè¯æ¨¡å‹ (Step {self.global_step})...")

        for batch_idx, batch in enumerate(tqdm(self.val_loader, desc="Validation")):
            if batch is None:
                continue

            try:
                ref_image = batch['ref_image'].to(self.device, non_blocking=True)
                tar_image = batch['tar_image'].to(self.device, non_blocking=True)
                text_tokens = batch['text_tokens'].to(self.device, non_blocking=True)
                ref_components = batch['ref_components'].to(self.device, non_blocking=True)
                tar_components = batch['tar_components'].to(self.device, non_blocking=True)

                if ref_components is not None:
                    ref_components[:, :, 0] = torch.clamp(ref_components[:, :, 0].long(), 0, 19)
                if tar_components is not None:
                    tar_components[:, :, 0] = torch.clamp(tar_components[:, :, 0].long(), 0, 19)

                outputs = self.model(ref_image, tar_image, text_tokens, ref_components, tar_components)

                total_loss, loss_dict = self.compute_alignment_loss(outputs, batch)
                val_losses.append(total_loss.item())

                # æ”¶é›†ç›¸ä¼¼åº¦ç»Ÿè®¡
                visual_features = outputs['visual_features']
                text_features = outputs['text_features']

                v_t_sim = F.cosine_similarity(visual_features, text_features, dim=1).mean().item()
                similarity_stats['visual_text'].append(v_t_sim)

                if 'component_outputs' in outputs:
                    component_features = outputs['component_outputs']['change_features']

                    c_v_sim = F.cosine_similarity(component_features, visual_features, dim=1).mean().item()
                    similarity_stats['comp_visual'].append(c_v_sim)

                    c_t_sim = F.cosine_similarity(component_features, text_features, dim=1).mean().item()
                    similarity_stats['comp_text'].append(c_t_sim)

                # æ”¶é›†æŒ‡æ ‡
                metrics = {'val_loss': total_loss.item()}
                for k, v in loss_dict.items():
                    if torch.is_tensor(v):
                        metrics[f'val_{k}'] = v.item()
                all_metrics.append(metrics)

            except Exception as e:
                continue

        if not val_losses:
            return {'val_loss': float('inf'), 'alignment_score': 0.0}

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_metrics = {}
        if all_metrics:
            for key in all_metrics[0].keys():
                values = [m[key] for m in all_metrics if key in m]
                if values:
                    avg_metrics[key] = np.mean(values)

        avg_val_loss = np.mean(val_losses)

        # è®¡ç®—å¯¹é½åˆ†æ•°
        alignment_similarity = 0.0
        sim_counts = 0
        for sim_type, values in similarity_stats.items():
            if values:
                alignment_similarity += np.mean(values)
                sim_counts += 1

        if sim_counts > 0:
            alignment_similarity /= sim_counts
            alignment_score = alignment_similarity
        else:
            alignment_score = 1.0 / (avg_val_loss + 1e-8)

        avg_metrics['val_loss'] = avg_val_loss
        avg_metrics['alignment_score'] = alignment_score

        # æ·»åŠ ç›¸ä¼¼åº¦ç»Ÿè®¡
        for sim_type, values in similarity_stats.items():
            if values:
                avg_metrics[f'{sim_type}_similarity'] = np.mean(values)

        # æ‰“å°éªŒè¯ç»“æœ
        print(f"\néªŒè¯ç»“æœ (Step {self.global_step}):")
        print(f"  éªŒè¯æŸå¤±: {avg_val_loss:.4f}")
        print(f"  å¯¹é½åˆ†æ•°: {alignment_score:.4f}")

        if 'visual_text_similarity' in avg_metrics:
            print(f"\n  ç›¸ä¼¼åº¦:")
            print(f"    è§†è§‰-æ–‡æœ¬: {avg_metrics['visual_text_similarity']:.4f}")
            if 'comp_visual_similarity' in avg_metrics:
                print(f"    ç»„ä»¶-è§†è§‰: {avg_metrics['comp_visual_similarity']:.4f}")
            if 'comp_text_similarity' in avg_metrics:
                print(f"    ç»„ä»¶-æ–‡æœ¬: {avg_metrics['comp_text_similarity']:.4f}")

        print(f"\n  æŸå¤±åˆ†è§£:")
        print(f"    å¯¹é½é¢„æµ‹: {avg_metrics.get('val_alignment', 0):.4f}")
        print(f"    è§†è§‰-æ–‡æœ¬: {avg_metrics.get('val_visual_text', 0):.4f}")
        print(f"    å¯¹æ¯”å­¦ä¹ : {avg_metrics.get('val_contrastive', 0):.4f}")

        return avg_metrics

    def save_checkpoint(self, is_best: bool = False, suffix: str = ""):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        try:
            checkpoint = {
                'epoch': self.current_epoch,
                'global_step': self.global_step,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'scheduler_state_dict': self.scheduler.state_dict(),
                'best_val_loss': self.best_val_loss,
                'best_alignment_score': self.best_alignment_score,
                'loss_weights': self.loss_weights,
                'config': self.config.__dict__
            }

            if suffix:
                checkpoint_path = self.checkpoint_dir / f"checkpoint_{suffix}.pt"
            else:
                checkpoint_path = self.checkpoint_dir / f"checkpoint_epoch_{self.current_epoch}_step_{self.global_step}.pt"

            torch.save(checkpoint, checkpoint_path)

            if is_best:
                best_path = self.checkpoint_dir / "best_model.pt"
                torch.save(checkpoint, best_path)
                print(f"ğŸ‰ ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")

        except Exception as e:
            print(f"ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")

    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print(f"\n{'=' * 60}")
        print(f"å¼€å§‹ç¬¬äºŒé˜¶æ®µå¯¹é½è®­ç»ƒï¼Œå…± {self.config.stage2_epochs} ä¸ªepoch")
        print(f"{'=' * 60}")

        start_time = time.time()

        try:
            for epoch in range(self.current_epoch, self.config.stage2_epochs):
                self.current_epoch = epoch
                epoch_start_time = time.time()

                print(f"\n{'=' * 50}")
                print(f"Epoch {epoch + 1}/{self.config.stage2_epochs}")
                print(f"{'=' * 50}")

                # è®­ç»ƒ
                train_losses = self.train_epoch(epoch)

                if train_losses:
                    print(f"\nè®­ç»ƒå®Œæˆ:")
                    print(f"  æ€»æŸå¤±: {train_losses.get('total', 0):.4f}")
                    for key in ['visual_text', 'comp_visual', 'comp_text']:
                        if key in train_losses:
                            print(f"  {key}: {train_losses[key]:.4f}")

                # éªŒè¯
                val_metrics = self.validate()

                # æ—©åœæ£€æŸ¥
                if val_metrics['alignment_score'] > self.best_alignment_score:
                    self.best_alignment_score = val_metrics['alignment_score']
                    self.best_val_loss = val_metrics['val_loss']
                    self.patience_counter = 0
                    self.save_checkpoint(is_best=True, suffix="best")
                    print(f"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹ï¼å¯¹é½åˆ†æ•°: {self.best_alignment_score:.4f}")
                else:
                    self.patience_counter += 1

                    if self.patience_counter >= self.max_patience // 2:
                        for param_group in self.optimizer.param_groups:
                            param_group['lr'] *= 0.5

                # æ—©åœ
                if self.patience_counter >= self.max_patience:
                    print(f"\nâš ï¸ æ—©åœè§¦å‘")
                    break

                # ä¿å­˜æ£€æŸ¥ç‚¹
                if (epoch + 1) % 5 == 0:
                    self.save_checkpoint(suffix=f"epoch_{epoch + 1}")

                # è®¡ç®—æ—¶é—´
                epoch_time = time.time() - epoch_start_time
                remaining_epochs = self.config.stage2_epochs - epoch - 1
                remaining_time = epoch_time * remaining_epochs

                print(f"\nEpoch {epoch + 1} æ—¶é—´: {epoch_time:.1f}s")
                print(f"é¢„è®¡å‰©ä½™æ—¶é—´: {remaining_time / 60:.1f}åˆ†é’Ÿ")
                print(f"{'=' * 50}")

        except KeyboardInterrupt:
            print("\nè®­ç»ƒè¢«ä¸­æ–­")
            self.save_checkpoint(suffix="interrupted")
        except Exception as e:
            print(f"\nè®­ç»ƒå‡ºé”™: {e}")
            self.save_checkpoint(suffix="error")
        finally:
            self.save_checkpoint(suffix="final")

        # è®­ç»ƒæ€»ç»“
        total_time = time.time() - start_time
        print(f"\n{'=' * 60}")
        print("è®­ç»ƒæ€»ç»“:")
        print(f"  æ€»æ—¶é—´: {total_time / 60:.1f} åˆ†é’Ÿ")
        print(f"  æ€»æ­¥æ•°: {self.global_step}")
        print(f"  æœ€ä½³å¯¹é½åˆ†æ•°: {self.best_alignment_score:.4f}")
        print(f"{'=' * 60}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='ç¬¬äºŒé˜¶æ®µè®­ç»ƒï¼šè§†è§‰-æ–‡æœ¬-ç»„ä»¶å¯¹é½')
    parser.add_argument('--stage1-checkpoint', type=str,
                        default=str(config.output_dir / "stage1" / "checkpoints" / "best_model.pt"),
                        help='Stage1æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--resume', type=str, help='ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ')
    parser.add_argument('--no-wandb', action='store_true', default=True, help='ç¦ç”¨wandb')
    parser.add_argument('--epochs', type=int, default=None, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=None, help='batch size')

    args = parser.parse_args()

    # æ›´æ–°é…ç½®
    if args.epochs:
        config.stage2_epochs = args.epochs
    if args.batch_size:
        config.batch_size = args.batch_size

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Stage2AlignmentTrainer(args.stage1_checkpoint, use_wandb=not args.no_wandb)

    # å¼€å§‹è®­ç»ƒ
    trainer.train()


if __name__ == "__main__":
    main()